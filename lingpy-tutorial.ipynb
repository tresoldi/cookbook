{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LingPy tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is LingPy?\n",
    "\n",
    "LingPy is a suite of open-source Python modules for sequence comparison, distance analyses, data operations and visualization methods in quantitative historical linguistics. The main idea of LingPy is to provide a software package which, on the one hand, integrates different methods for data analysis in quantitative historical linguistics within a single framework, and, on the other hand, serves as an interface for the preparation and analysis of linguistic data using biological software packages. [LINGPY] The main idea behind it is to combine different approaches to historical linguistics and evolutionary biology into a framework that closely models the most important aspects of the comparative method, allowing experts to focus in both theoretical and specific historical questions while the method takes care of the repetitive and schematic work.\n",
    "\n",
    "This tutorial illustrates how to use LingPy for the carrying out automatic tasks on historical linguistics: phonetic alignments from wordlists, cognate and borrowing detection, calculation of lexicostatistic distances and affinities among languages, and construction and visualization of language phylogenies. The instructions assume that both Python and LingPy have been properly installed, and that the user as intermediate or advanced proficiency with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## On historical linguistics\n",
    "\n",
    "The comparative method (Meiller 1925 [1954], Weiss 2014) has sucessfully elucidated the history of a wide range of language families of varying size and age (Baldi 1990, Campbell & Poser 2008) and external evidence has ofter confirmed the validity of the findings (McMahon & McMahon 2005: 10-14). The comparative method is not just a simple technique, but rather an *overarching framework* to study language history (Fox 1995, Jarceva 1990, Klimov 1990, Ross & Durie 1996), which is supported by an underlying workflow that scholas implicitly follow. The most crucial part is the identification of *cognate words* and regular *sound correspondences*. The *iterative character* of the workflow requires repetition in all steps. Iteration is important to address circularity problems: *cognate words* can, for example, only be identified with the help of regular *sound correspondences*, but sound correspondences themselves occur only in cognate words. An iterative procedure circumvents this problem by starting with an initial hypothesis regarding sound correspondences and cognate words which is the constantly revised. [CALC-PROJECT, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## How to load LingPy\n",
    "\n",
    "Assuming one has a working Python and LingPy installation, the software can be loaded either in a script file or in the interactive shell using the standard procedures for loading libraries in Python. Here we demonstrate how to load the library; loading it as `from lingpy import *` is not recommended, as it pollutes the namespace and make maintainance harder; if you prefer a less-verbose alternative, we suggest loading it with `import lingpy as lp`. Please remember that the first time you load the library it will probably show a lot of information about parameters and models being loaded and cached; this is intentional and expected. The path to the cached data depends on your system, but in common Linux setups it will probably reside in the `~/.cache/lingpy` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lingpy name:  lingpy\n",
      "lingpy version:  2.5\n",
      "lingpy author:  Johann-Mattis List, Robert Forkel (with contributions by Steven Moran, Taraka Rama, Johannes Dellert, Frank Nagel, Peter Bouda, and Simon Greenhill)\n",
      "lingpy date:  2016-21-04\n",
      "lingpy path:  ['/home/tiago/lingpy/lingpy']\n"
     ]
    }
   ],
   "source": [
    "import lingpy\n",
    "from lingpy.tests.util import test_data # used in this tutorial\n",
    "\n",
    "# query some properties from the library  confirming that it was properly loaded;\n",
    "# please remember that your output might be different (for example, in the path\n",
    "# of installation) and that this is not needed in production code, but might\n",
    "# help debugging\n",
    "print('lingpy name: ', lingpy.__name__)\n",
    "print('lingpy version: ', lingpy.__version__)\n",
    "print('lingpy author: ', lingpy.__author__)\n",
    "print('lingpy date: ', lingpy.__date__)\n",
    "print('lingpy path: ', lingpy.__path__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Your first alignments\n",
    "\n",
    "Although less common in traditional historical linguistics, phonetic alignment plays a crucial role in automatic approaches, with alignment analyses being currently used in many different subfields, such as dialectology (Prokić et al., 2009), phylogenetic reconstruction (Holman et al., 2011) and cognate detection (List, 2012a). Furthermore, alignment analyses are very useful for data visualization, since they directly show which sound segments correspond in cognate words. [LIST2013]\n",
    "\n",
    "Before we dive into the normal usage of lingpy, with wordlists, multiple alignments or reconstruction of language phylogenies, it is valid to experiment with basic pairwise aligments to understand what could be considered the core of lingpy. Here we are only going to study how to use lingpy for performing such tasks, for detailed explanation on the inner workings and the theory behind such taks, please refer to the recommended bibliography, such as LIST2014.\n",
    "\n",
    "Pairwise alignment works on two sequences of phonetic units, which are not necessarily \"phonemes\" in their IPA representation. In fact, while a model that allows to compare sequences with detailed phonetic or phonological representations is possible, as we will see the ones distributed with lingpy don't compare sounds, but sound classes. The advantages and disadvantages of such approach are discussed in the suggest bibliography.\n",
    "\n",
    "Here we will run some tests by manually loading the traditional Needleman-Wunsch alignment algorithm and defining a function that gives us a better representation of the alignment. This is not needed in actual research because, as we will see, there are lingpy classes that do this task in a better why, but it is valid while we are understanding how this works. We are going to use two different sets of words: the first is a basic `cat`/`fat` comparison, the second is a more complex and invented data. The sequences in this last set are distinguished by:\n",
    "\n",
    "- an epenthetic vowel missing in one of the sequences;\n",
    "- a different place of articulation for the first consonant;\n",
    "- a different roundness for the first vowel;\n",
    "- the voiceness of the last consonant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq a:  c a t\n",
      "seq b:  f a t\n",
      "score:  1.0\n",
      "\n",
      "seq a:  p i r - - a v\n",
      "seq b:  - i t y l a f\n",
      "score:  -3.0\n"
     ]
    }
   ],
   "source": [
    "# manually load the alignment algorithm\n",
    "from lingpy.align.pairwise import nw_align\n",
    "\n",
    "# define the two sets for pairwise alignment\n",
    "seqA1 = 'cat'\n",
    "seqA2 = 'fat'\n",
    "\n",
    "seqB1 = 'pirav'\n",
    "seqB2 = 'itylaf'\n",
    "\n",
    "# prints a representation of the result of a pairwise alignment\n",
    "# the alignment functions return a tuple with three values: the alignment for the first sequence,\n",
    "# the alignment for the second sequence, and the alignment score; the alignment for the first and second sequence\n",
    "# can be either lists of phonetic units or lists of lists of phonetic units (that is, list of groups of units)\n",
    "def print_align(align, new_line=False):\n",
    "    if type(align[0][0]) is str:\n",
    "        print('seq a: ', ' '.join(align[0]))\n",
    "        print('seq b: ', ' '.join(align[1]))\n",
    "    if type(align[0][0]) is list:\n",
    "        print('seq a: ', '\\t\\t'.join([' '.join(e) for e in align[0]]))\n",
    "        print('seq b: ', '\\t\\t'.join([' '.join(e) for e in align[1]]))\n",
    "        \n",
    "    print('score: ', align[2])\n",
    "    \n",
    "    # print a new line, separating different alignments, if requested\n",
    "    if new_line:\n",
    "        print()\n",
    "    \n",
    "# shows the nw alignment for the first set\n",
    "print_align(nw_align(seqA1, seqA2), new_line=True)\n",
    "\n",
    "# shows the nw alignment for the second set\n",
    "print_align(nw_align(seqB1, seqB2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see the output of an alignment algorithm: each unit in a sequence is mapped to either a corresponding unit in the other sequence (which may or may not be identical), or to a gap represented by a dash. The algorithm performed as expected for the \"cat\"/\"fat\" set, but its alignment in the test case, while not impossible, is not what was expected.\n",
    "\n",
    "There are many ways to tweak the workings of the alignment algorithms, including a simpler one such as Needleman-Wunsch. One important value to set is the gap penalty, which defaults to -1. Let's see how the algorithm performs when a larger gap is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq a:  p i r - a v\n",
      "seq b:  i t y l a f\n",
      "score:  -8.0\n"
     ]
    }
   ],
   "source": [
    "# shows the nw alignment for the second set, with a larger gap penalty\n",
    "print_align(nw_align(seqB1, seqB2, gap=-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The algorithm perfomed even worse, with an unlikely matching of consonants and vowels. Let's see how it performs with a smaller gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq a:  p i r - - - a v -\n",
      "seq b:  - i - t y l a - f\n",
      "score:  1.2999999999999998\n"
     ]
    }
   ],
   "source": [
    "# shows the nw alignment for the second set, with a smaller gap penalty\n",
    "print_align(nw_align(seqB1, seqB2, gap=-0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When there is almost no penalty for a gap, the algorithm will likely only match identical units, even when they don't necessarly correspond (remember that, in our test, the `/i/` if \"pirav\" should match the `/y/` in \"itylaf\", and not the epenthetic vowel).\n",
    "\n",
    "There is a function better suited for these pairwise tests, which allows us not only to select between different alignment modes (as detailed in LIST2012a, using \"global\" as the default mode), but also to specify different penalty scores for gap opening (`gop`, which defaults to -1) and for gap extension scale (`scale`, which defaults to 0.5). Let's experiment with both our sets with the four different alignment modes, using the default values for gap opening and for gap extension scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq a:  p i - - r a v\n",
      "seq b:  - i t y l a f\n",
      "score:  -2.0\n",
      "\n",
      "seq a:  p i r\t\ta\t\tv\n",
      "seq b:  i t y l\t\ta\t\tf\n",
      "score:  1.0\n",
      "\n",
      "seq a:  p i r a v - - - - -\n",
      "seq b:  - i - - - t y l a f\n",
      "score:  1.0\n",
      "\n",
      "seq a:  p i r a v - - - - - -\n",
      "seq b:  - - - - - i t y l a f\n",
      "score:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# manually load the alignment algorithm\n",
    "from lingpy.align.pairwise import pw_align\n",
    "\n",
    "# shows the pw alignment for the second set, with the (default) global mode\n",
    "print_align(pw_align(seqB1, seqB2), new_line=True)\n",
    "\n",
    "# shows the pw alignment for the second set, with the local mode\n",
    "print_align(pw_align(seqB1, seqB2, mode=\"local\"), new_line=True)\n",
    "\n",
    "# shows the pw alignment for the second set, with the dialign mode\n",
    "print_align(pw_align(seqB1, seqB2, mode=\"dialign\"), new_line=True)\n",
    "\n",
    "# shows the pw alignment for the second set, with the overlap mode\n",
    "print_align(pw_align(seqB1, seqB2, mode=\"overlap\"), new_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some alignments, such as in \"local\" above, return groups of phonetic units and might be more adequade. Do not be fooled by the supposedly useless results of \"dialign\" and \"overlap\" modes here: they are performing poorly not only because of gap opening and extension penalties, but particularly due to the fact that we are not using sound classes and neither providing the algorithm with a scorer. This is intentional, as you can probably understand, even without a true description, what is favored by each mode when aligning sequences.\n",
    "\n",
    "It is time to understand how the sound models used by lingpy work, in order to proceed with pairwise and multiple alignments. Before that, it is valid to know that lingpy offers a set of helper functions to deal with phonetic representations and prosodic sequences. The two most important are `sampa2uni()`, which converts from an X-SAMPA to an Unicode representation of IPA, and `ipa2tokens()`, which tokenizes IPA-encoded strings while taking care of details such diacritics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʦɔyɡə\n",
      "['ʦ', 'ɔy', 'ɡ', 'ə']\n"
     ]
    }
   ],
   "source": [
    "ipa = lingpy.sampa2uni('tsOyg@')\n",
    "tokens = lingpy.ipa2tokens(ipa)\n",
    "\n",
    "print(ipa)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sound classes\n",
    "\n",
    "Sound classes go back to an approach Dolgopolsky1964. The basic idea behind sound classes is to reduce the IPA space of possible phonetic segments in order to guarantee both the comparability of sounds between languages, but also to give some assessment regarding the probability that sounds belonging to the same class occur in correspondence relations in genetically related languages. More recently, sound classes have been applied in a couple of approaches, including phonetic alignment (see List2012a), and automatic cognate detection (see Turchin2012, List2012b).\n",
    "\n",
    "Besides `_color`, a model which is used for the coloring of sound-tokens when creating html-output, and `art`, a model which is used for the calculation of sonority profiles and prosodic strings (see List2012), lingpy includes three different sound models that can be used as a starting point for your research:\n",
    "\n",
    "- `sca` - the SCA sound-class model (see List2012a);\n",
    "- `dolgo` - the DOLGO sound-class model (see: Dolgopolsky1986);\n",
    "- `asjp` - the ASJP sound-class model (see Brown2008 and Brown2011).\n",
    "\n",
    "We will experiment mostly with `sca`, which should be your default. Please remember that it is also possible to specify a user-defined model, which you might need when you advance with your research.\n",
    "\n",
    "Models are loaded from lingpy resources with the `lingpy.rc()` function and have two essential attributes: a `converter`, used to map IPA-tokens to sound classes, and a `scorer`, used to map two sound classes to a given score used in the alignment (in a simplified and somewhat wrong description, it indicates the tendency for a given sound class to correspond to the other).\n",
    "\n",
    "Here we show how to load the standard models and how different they are in their conversion of sounds to sound classes (`converter`) and looking at some example of scores for comparing sound classes. You can see that lingpy tends to favor vowels in its alignment (or, more precisely, classes with a tendency to high prosody score), for reasons discussed in List2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(asjp) /a/ > class 'a' / -8.31, -8.31, -21.88\n",
      "(sca) /a/ > class 'A' / -10.0, -10.0, -10.0\n",
      "(dolgo) /a/ > class 'V' / -10.0, -10.0, -20.0\n",
      "\n",
      "(asjp) /b/ > class 'b' / 4.38, 0.44, -21.88\n",
      "(sca) /b/ > class 'P' / 10.0, 0.0, -10.0\n",
      "(dolgo) /b/ > class 'P' / 10.0, 0.0, -20.0\n",
      "\n",
      "(asjp) /s/ > class 's' / 0.0, 0.44, -21.88\n",
      "(sca) /s/ > class 'S' / 0.0, 0.0, -10.0\n",
      "(dolgo) /s/ > class 'S' / 0.0, 0.0, -20.0\n",
      "\n",
      "(asjp) /n/ > class 'n' / 0.0, 1.31, -21.88\n",
      "(sca) /n/ > class 'N' / 0.0, 0.0, -10.0\n",
      "(dolgo) /n/ > class 'N' / 0.0, 0.0, -20.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loads the standard models from lingpy resources\n",
    "asjp = lingpy.rc('asjp')\n",
    "sca = lingpy.rc('sca')\n",
    "dolgo = lingpy.rc('dolgo')\n",
    "\n",
    "# shows the different classes to which some sounds are mapped in each model, and the\n",
    "# score of such class in the model when compared with the class for the\n",
    "# sound /p/, for the sound /l/, and with class '_' (i.e., a gap)\n",
    "for sound in ['a', 'b', 's', 'n']:\n",
    "    for model in [asjp, sca, dolgo]:\n",
    "        print(\"({0}) /{1}/ > class '{2}' / {3}, {4}, {5}\".format(model.name,\n",
    "            sound,\n",
    "            model.converter[sound],\n",
    "            model.scorer[model.converter[sound], model.converter['p']],\n",
    "            model.scorer[model.converter[sound], model.converter['l']],\n",
    "            model.scorer[model.converter[sound], '_']))\n",
    "    print() # separate with newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pairwise alignment with sound classes\n",
    "\n",
    "We can now experiment with the alignments from the previous section by mapping the sounds of our words to their sound classes and running the alignment algorithms a second time. Please note that this is only using the `converter` from the sound model, but not the scorer: in computational terms, it is still assuming that each sound class is equally distinct from all other sound classes (in other words, the score is always the same). \n",
    "\n",
    "We use the `lingpy.ipa2tokens()` function to tokenize our strings and the `lingpy.tokens2class()` function to convert each token to its class in the `sca` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat -> ['C', 'A', 'T']\n",
      "fat -> ['B', 'A', 'T']\n",
      "pirav -> ['P', 'I', 'R', 'A', 'B']\n",
      "itylaf -> ['I', 'T', 'Y', 'L', 'A', 'B']\n",
      "seq a:  P I - - R A B\n",
      "seq b:  - I T Y L A B\n",
      "score:  0.0\n",
      "\n",
      "seq a:  P I R\t\tA B\t\t\n",
      "seq b:  I T Y L\t\tA B\t\t\n",
      "score:  2.0\n",
      "\n",
      "seq a:  P I R A B - - - - -\n",
      "seq b:  - I - - - T Y L A B\n",
      "score:  1.0\n",
      "\n",
      "seq a:  P I - - R A B\n",
      "seq b:  - I T Y L A B\n",
      "score:  0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert our sequences to lists of sound classes in the SCA model\n",
    "seqA1_sc = lingpy.tokens2class(lingpy.ipa2tokens(seqA1), 'sca')\n",
    "seqA2_sc = lingpy.tokens2class(lingpy.ipa2tokens(seqA2), 'sca')\n",
    "seqB1_sc = lingpy.tokens2class(lingpy.ipa2tokens(seqB1), 'sca')\n",
    "seqB2_sc = lingpy.tokens2class(lingpy.ipa2tokens(seqB2), 'sca')\n",
    "\n",
    "# show the results of conversions\n",
    "print('{0} -> {1}'.format(seqA1, seqA1_sc))\n",
    "print('{0} -> {1}'.format(seqA2, seqA2_sc))\n",
    "print('{0} -> {1}'.format(seqB1, seqB1_sc))\n",
    "print('{0} -> {1}'.format(seqB2, seqB2_sc))\n",
    "\n",
    "# shows the pw alignment for the second set (with sound classes), with the (default) global mode\n",
    "print_align(pw_align(seqB1_sc, seqB2_sc), new_line=True)\n",
    "\n",
    "# shows the pw alignment for the second set (with sound classes), with the local mode\n",
    "print_align(pw_align(seqB1_sc, seqB2_sc, mode=\"local\"), new_line=True)\n",
    "\n",
    "# shows the pw alignment for the second set (with sound classes), with the dialign mode\n",
    "print_align(pw_align(seqB1_sc, seqB2_sc, mode=\"dialign\"), new_line=True)\n",
    "\n",
    "# shows the pw alignment for the second set (with sound classes), with the overlap mode\n",
    "print_align(pw_align(seqB1_sc, seqB2_sc, mode=\"overlap\"), new_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While there are still some problems, we can immediately see that using soundclasses has led to some improvements in the alignment. It should also be possible to understand how a scorer which considers the different distances among sound classes should improve even further the results in some cases, for example with diagonal mode alignment.\n",
    "\n",
    "LingPy offers a class to automatically apply a model when performing pairwise alignment, similar to what we did above. It also allows to set some properties for the alignment (such as `gop` for the gap opening penalty and `factor` for the factor by which matches in identical prosodic position are increased) and allows to print the results without our ad-hoc `print_align()` method.\n",
    "\n",
    "Here we test the results of the alignment with different methods, properties, and modes. You are encouraged to explore the results, to tweak it, and to perform tests in words of your choice (remember that `lingpy.sampa2uni()` can help you with IPA input, if needed). Pay attention to the reported alignment score, particularly when the results are apparently the same, and to alignment methods that exclude some sounds from the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard alignment with SCA:\n",
      "p\ti\t-\t-\tr\ta\tv\n",
      "-\ti\tt\ty\tl\ta\tf\n",
      "27.6\n",
      "\n",
      "Standard alignment with ASJP:\n",
      "p\ti\t-\t-\tr\ta\tv\n",
      "-\ti\tt\ty\tl\ta\tf\n",
      "35.647\n",
      "\n",
      "Standard alignment with SCA, high gap opening penalty:\n",
      "-\tp\ti\tr\ta\tv\n",
      "i\tt\ty\tl\ta\tf\n",
      "13.7\n",
      "\n",
      "Standard alignment with SCA, low gap opening penalty:\n",
      "p\ti\t-\t-\tr\ta\tv\n",
      "-\ti\tt\ty\tl\ta\tf\n",
      "30.84\n",
      "\n",
      "Standard alignment with SCA, high gap extension penalty:\n",
      "-\tp\ti\tr\ta\tv\n",
      "i\tt\ty\tl\ta\tf\n",
      "22.7\n",
      "\n",
      "Standard alignment with SCA, low gap extension penalty:\n",
      "p\ti\t-\t-\tr\ta\tv\n",
      "-\ti\tt\ty\tl\ta\tf\n",
      "29.44\n",
      "\n",
      "Standard alignment with SCA, high factor of identical prosodic position increase:\n",
      "p\ti\t-\t-\tr\ta\tv\n",
      "-\ti\tt\ty\tl\ta\tf\n",
      "44.4\n",
      "\n",
      "Standard alignment with SCA, low factor of identical prosodic position increase:\n",
      "-\tp\ti\tr\ta\tv\n",
      "i\tt\ty\tl\ta\tf\n",
      "21.689999999999998\n",
      "\n",
      "Standard alignment with SCA, local alignment mode:\n",
      "p\ti\tr\ta\tv\n",
      "t\ty\tl\ta\tf\n",
      "29.3\n",
      "\n",
      "Standard alignment with SCA, diagonal alignment mode:\n",
      "p\ti\t-\t-\tr\ta\tv\n",
      "-\ti\tt\ty\tl\ta\tf\n",
      "29.3\n",
      "\n",
      "Standard alignment with SCA, overlap alignment mode:\n",
      "-\tp\ti\tr\ta\tv\n",
      "i\tt\ty\tl\ta\tf\n",
      "29.3\n"
     ]
    }
   ],
   "source": [
    "from lingpy.align.pairwise import Pairwise\n",
    "\n",
    "pw_aligner = Pairwise(seqB1, seqB2)\n",
    "\n",
    "print(\"Standard alignment with SCA:\")\n",
    "pw_aligner.align(model='sca', pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with ASJP:\")\n",
    "pw_aligner.align(model='asjp', pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, high gap opening penalty:\")\n",
    "pw_aligner.align(model='sca', gop=-10, pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, low gap opening penalty:\")\n",
    "pw_aligner.align(model='sca', gop=-0.1, pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, high gap extension penalty:\")\n",
    "pw_aligner.align(model='sca', scale=2, pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, low gap extension penalty:\")\n",
    "pw_aligner.align(model='sca', scale=0.1, pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, high factor of identical prosodic position increase:\")\n",
    "pw_aligner.align(model='sca', factor=1, pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, low factor of identical prosodic position increase:\")\n",
    "pw_aligner.align(model='sca', factor=0.01, pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, local alignment mode:\")\n",
    "pw_aligner.align(model='sca', mode='local', pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, diagonal alignment mode:\")\n",
    "pw_aligner.align(model='sca', mode='dialign', pprint=True)\n",
    "\n",
    "print(\"\\nStandard alignment with SCA, overlap alignment mode:\")\n",
    "pw_aligner.align(model='sca', mode='overlap', pprint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multiple alignment\n",
    "\n",
    "The next obvious step from pairwise alignment is multiple alignment, which is what you are going to deal with when researching language families and candidates for cognancy. As in the case with pairwise alignemnt, lingpy offers a class that allows you to quickly check the results for the alignment of sets of multiple words.\n",
    "\n",
    "Let's start by exploring how lingpy aligns names related to a famous character of the Harry Potter lore. We are intentionally including two names (\"Tom Riddle\" and \"Harry\") that clearly are not cognates, so that we can use the same set later when exploring cognancy. We are aware that the strings here presented are not valid IPA transcriptions of the pronounciation of the names in most languages and that there are inconsistencies: it is good enough for our demonstration, and using plain ASCII characters simplies the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\to\tl\t-\td\te\tm\to\tr\t-\tt\t-\t-\n",
      "w\ta\tl\t-\td\te\tm\ta\tr\t-\t-\t-\t-\n",
      "w\t-\tl\ta\td\ti\tm\ti\tr\t-\t-\t-\t-\n",
      "v\t-\tl\ta\td\ty\tm\ty\tr\t-\t-\t-\t-\n",
      "-\t-\t-\t-\tt\to\tm\t-\tr\ti\tdd\tl\te\n",
      "-\t-\t-\t-\th\ta\t-\t-\trr\t-\t-\t-\ty\n"
     ]
    }
   ],
   "source": [
    "hp_names = ['woldemort','waldemar','wladimir','vladymyr', 'tomriddle', 'harry']\n",
    "msa = lingpy.Multiple(hp_names)\n",
    "msa.prog_align()\n",
    "print(msa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The results are very good if we consider that only the first four words are candidates for cognancy. Still, the algorithm was able to align in the most acceptable way the two outliers: for example, the double /d/ of \"Tom Riddle\" was aligned with the voiceless /t/ of \"Woldemort\".\n",
    "\n",
    "One aspect that might draw your attention is the alignment of the first sounds in the cognates: the fact that you either have a vowel before or after the /l/ would be takes in historical linguistics as a strong indicator for metathesis. This sound change is, in fact, one of the hard parts for both automatic and manual sound alignment and will not be covered here (the impact of this difficulty in actual studies is not as strong as it may seem at first sight). If you are interested, you can study the discussion on the topic in LIST2014, and be advised that lingpy offers `swap_check()` a method to check for the possibility of swapped sited in the alignment, as shown below using a restricted version of our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swap_check() in restricted:  True\n",
      "swap_check() in full:  False\n"
     ]
    }
   ],
   "source": [
    "# restricted dataset with high potential case for site swapping\n",
    "swap_aligner = lingpy.Multiple([\"woldemort\", \"waldemar\", \"wladimir\"])\n",
    "swap_aligner.prog_align()\n",
    "swap_aligner.swap_check()\n",
    "\n",
    "# prints the analysis of `swap_check()` in the restricted and in the\n",
    "# full dataset\n",
    "print('swap_check() in restricted: ', swap_aligner.swap_check())\n",
    "print('swap_check() in full: ', msa.swap_check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic formats\n",
    "\n",
    "After understanding the basic properties of pairwise and multiple alignment, the building block of lingpy, we can start looking at real data and conducting real analysis.\n",
    "\n",
    "The basic input format read by LingPy is a comma or tab delimited text file in which the first line (the header) indicates the values of the columns and all words are listed in the following rows. The format is very flexible. No specific order of columns or rows is required. Any additional data can be specified by the user, as long as it is in a separate column. Each row represents a word that has to be characterized by a minimum of four values that are given in separate columns: (1) ID, an integer that is used to uniquely identify the word during calculations, (2) CONCEPT, a gloss which indicates the meaning of the word and which is used to align the words semantically, (3) WORD, the orthographic representation of the word, and (4) DOCULECT, the name of the language (or dialect) in which the word occurs. Basic output formats are essentially the same, the difference being that the results of calculations are added as separate columns. [LIST2013]\n",
    "\n",
    "### Wordlist\n",
    "\n",
    "For the Wordlist class (and also for all classes that inherit from it, such as LexStat, PhyBo, Alignments), a simple csv-format is used. This format is a simple comma or tab delimited text file in which the header specifies all entry types in a given dataset. This format can be further extended by adding key-value pairs in the lines before the header, such as, for example, information regarding the author, the data, or general notes.\n",
    "\n",
    "In the example below, we have 12 different entries (\"words\") that express four different semantic groups or \"concepts\" (\"hand\", \"leg\", \"Woldemort\", and \"Harry\") in 4 different languages (German, English, Russian, and Ukrainian). The \"counterpart\" is a representation of the entry, and in most cases it will be its written form in a script used by its language, and IPA, as expected, is a phonemic representation of the pronunciation (as for why it is better to use a phonemic and not a phonetic representation, please refer to the essential bibliography). The cognate ID groups entries by a shared common ancestor, with unique identification numbers for unique ancestors of a concept. So, for example, German `\"Hand\"` and English `\"hand\"` share a cognate ID due to both descending from Proto-Germanic `*handuz`, and Russian `\"рука\"` and Ukrainian `\"рука\"` share a different cognate ID due to its descending from Proto-Slavic `*rǫka`; at the same time, all entries for concept \"Woldemort\" share a cognate ID as they all descend (in some cases due to borrowing). It is important to know that an entry might be the only representative of its cognate set, and in fact this is common (see the case of entry number 5, German `\"Bein\"`). \n",
    "\n",
    "```\n",
    "ID   CONCEPT     COUNTERPART   IPA         DOCULECT     COGID\n",
    "1    hand        Hand          hant        German       1\n",
    "2    hand        hand          hænd        English      1\n",
    "3    hand        рука          ruka        Russian      2\n",
    "4    hand        рука          ruka        Ukrainian    2\n",
    "5    leg         Bein          bain        German       3\n",
    "6    leg         leg           lɛg         English      4\n",
    "7    leg         нога          noga        Russian      5\n",
    "8    leg         нога          noha        Ukrainian    5\n",
    "9    Woldemort   Waldemar      valdemar    German       6\n",
    "10   Woldemort   Woldemort     wɔldemɔrt   English      6\n",
    "11   Woldemort   Владимир      vladimir    Russian      6\n",
    "12   Woldemort   Володимир     volodimir   Ukrainian    6\n",
    "9    Harry       Harald        haralt      German       7\n",
    "10   Harry       Harry         hæri        English      8\n",
    "11   Harry       Гарри         gari        Russian      8\n",
    "12   Harry       Гаррi         hari        Ukrainian    8\n",
    "```\n",
    "\n",
    "This are exactly the contents of file `toy_wordlist.wl` distibuted along with this tutorial and loaded, in the code below, with function `get_wordlist()`. Some queries demonstrate that the list was loaded correctly; please remember that, while it obviously does not try to replace a database system, the Wordlist class offers many helper function to deal with the data, such as `add_entries()`, `renumber()`, and `output()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 5, 1, 13]\n",
      "defaultdict(<class 'list'>, {'Woldemort': [11], 'leg': [7], 'hand': [3], 'Harry': [15]})\n"
     ]
    }
   ],
   "source": [
    "from lingpy.basic.wordlist import get_wordlist\n",
    "\n",
    "wl = get_wordlist(test_data('tutorial.qlc'), delimiter='\\t')\n",
    "\n",
    "print(wl.get_list(language=\"German\"))\n",
    "print(wl.get_dict(language=\"Russian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pairwise phonetic alignments\n",
    "\n",
    "The input format for text files containing unaligned sequence pairs is called PSQ-format. Files in this format should have the extension psq. The first line of a PSQ-file contains information regarding the dataset. The sequence pairs are given in triplets, with a sequence identifier in the first line of a triplet (containing the meaning, or orthographical information) and the two sequences in the second and third line, whereas the first column of each sequence line contains the name of the taxon and the second column the sequence in IPA format; columns are separated by tabs, sounds by spaces. All triplets are divided by one empty line. As an example, consider the file `harry_potter.psq`:\n",
    "\n",
    "```\n",
    "Harry Potter Testset\n",
    "Woldemort in German and Russian\n",
    "German      w a l d e m a r\n",
    "Russian     v l a d i m i r\n",
    " \n",
    "Woldemort in English and Russian\n",
    "English     w o l d e m o r t\n",
    "Russian     v l a d i m i r\n",
    " \n",
    "Woldemort in English and German\n",
    "English     w o l d e m o r t\n",
    "German      w a l d e m a r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('w a l d e m a r', 'v l a d i m i r'), ('w o l d e m o r t', 'v l a d i m i r'), ('w o l d e m o r t', 'w a l d e m a r')]\n"
     ]
    }
   ],
   "source": [
    "from lingpy.align.sca import PSA\n",
    "\n",
    "psa = PSA(infile=test_data('harry_potter.psq'))\n",
    "\n",
    "print(psa.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The output counterpart of the PSQ-format is the PSA-format. It is a specific format for text files containing already aligned sequence pairs. Files in this format should have the extension psa. The first line of a PSA-file contains information regarding the dataset. The sequence pairs are given in triplets, with a sequence identifier in the first line of a triplet (containing the meaning, or orthographical information) and the aligned sequences in the second and third line, whith the name of the taxon in the first column and all aligned segments in the following columns, separated by tabstops. All triplets are divided by one empty line. As an example, consider the file `harry_potter.psa`:\n",
    "\n",
    "```\n",
    "Harry Potter Testset\n",
    "Woldemort in German and Russian\n",
    "German.     w     a     l     -     d     e     m     a     r\n",
    "Russian     v     -     l     a     d     i     m     i     r\n",
    " \n",
    "Woldemort in English and Russian\n",
    "English     w     o     l     -     d     e     m     o     r     t\n",
    "Russian     v     -     l     a     d     i     m     i     r     -\n",
    "\n",
    "Woldemort in English and German\n",
    "English     w     o     l     d     e     m     o     r     t\n",
    "German.     w     a     l     d     e     m     a     r     -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Multiple Alignments (MSQ and MSA)\n",
    "\n",
    "A specific format for text files containing multiple unaligned sequences is the MSQ-format. Files in this format should have the extension msq. The first line of an msq-file contains information regarding the dataset. The second line contains information regarding the sequence (meaning, identifier), and the following lines contain the name of the taxa in the first column and the sequences in IPA format in the second column, separated by a tabstop. As an example, consider the file harry_potter.msq:\n",
    "    \n",
    "```\n",
    "1 Harry Potter Testset\n",
    "2 Woldemort (in different languages)\n",
    "3 English     v o l d e m o r t\n",
    "4 German      w a l d e m a r\n",
    "5 Russian     v l a d i m i r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The msa-format is a specific format for text files containing already aligned sequence pairs. Files in this format should have the extension msa. The first line of a MSA-file contains information regarding the dataset. The second line contains information regarding the sequence (its meaning, the protoform corresponding to the cognate set, etc.). The aligned sequences are given in the following lines, whereas the taxa are given in the first column and the aligned segments in the following columns. Additionally, there may be a specific line indicating the presence of swaps and a specific line indicating highly consistent sites (local peaks) in the MSA. The line for swaps starts with the headword SWAPS whereas a plus character (+) marks the beginning of a swapped region, the dash character (-) its center and another plus character the end. All sites which are not affected by swaps contain a dot. The line for local peaks starts with the headword LOCAL. All sites which are highly consistent are marked with an asterisk (*), all other sites are marked with a dot (.). As an example, consider the file harry_potter.msa:\n",
    "\n",
    "```\n",
    "Harry Potter Testset\n",
    "Woldemort (in different languages)\n",
    "English     v     o     l     -     d     e     m     o     r     t\n",
    "German.     w     a     l     -     d     e     m     a     r     -\n",
    "Russian     v     -     l     a     d     i     m     i     r     -\n",
    "SWAPS..     .     +     -     +     .     .     .     .     .     .\n",
    "LOCAL.      *     *     *     .     *     *     *     *     *     .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### search automatically for cognates across multiple languages\n",
    "\n",
    "LingPy includes LexStat, a powerful method for automatic detection of cognates. In fact, while relying on the methods for pairwise and multiple comparison describe above, LexStat draws from traditional practices in historical linguistics. In general, the procedures for cognate detection in the traditional approach could be described in the following way:\n",
    "\n",
    "- Compile an initial list of putative cognate sets\n",
    "- Extract an initial list of putative sets of sound correspondences from the initial cognate list\n",
    "- Refine the cognate list and the correspondences list by\n",
    "    - adding and deleting cognate sets from the cognate list, depending on whether they are consistent with the correspondence list or not, and\n",
    "    - adding and deleting correspondence sets from the correspondence list, depending on whether they are consistent with the cognate list or not\n",
    "- Finish when the results are satisfying enough.\n",
    "\n",
    "The cognate detection with LexStat uses a theoretically similar model in four steps:\n",
    "\n",
    "- Sequence conversion, in which sequences are converted to sound classes and prosodic profiles (all sequences are internally represented both as sound classes and by prosodic strings, which indicate the prosodic environment of each phonetic segment: initial, ascending, maximum, descending, final)\n",
    "- Scoring-Scheme creation, in which language-specific scoring schemes are determined using a permutation method (global and pairwise alignment analyses of all sequence pairs occuring in the same semantic slot whose distance is beyong a certain threshold are stored, the wordlists are shuffled repeatedly and the average results are stored, so that log-odd scores from the distributions can be calculated)\n",
    "- Distance calculation, in which pairwise distances between sequences are calculated based on the language-specific scoring-scheme\n",
    "- Sequence clustering, in which sequences are clustered into cognate sets whose average distance is beyond a certain threshold\n",
    "\n",
    "Here, we are going to load a complex dataset with 200 concepts in English, German, French, Albanian, Turkish, Hawaiian, and Navajo (a Swadesh wordlist), and apply LexStat to search for cognates. The dataset is part of the standard lingpy distribution, and it includes a cognate identification column with can be used a gold standard for testing the perfomance of our automatic cognate identification.\n",
    "\n",
    "Remember that it is a good practice to set `check=True` when loading a dataset, so that you can catch errors early on, before they propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-11 18:11:32,481 [INFO] No obvious errors found in the data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of languages:  7\n",
      "number of concepts:  200\n",
      "number of entries:  1400\n",
      "{'Turkish': 200, 'English': 200, 'Hawaiian': 200, 'Albanian': 200, 'German': 200, 'Navajo': 200, 'French': 200}\n"
     ]
    }
   ],
   "source": [
    "# load the data and check its contents\n",
    "lex = lingpy.LexStat(test_data('KSL.qlc'), check=True)\n",
    "\n",
    "print('number of languages: ', lex.width)\n",
    "print('number of concepts: ', lex.height)\n",
    "print('number of entries: ', len(lex))\n",
    "\n",
    "# show coverage (a dataset might not have full coverage for some or all languages)\n",
    "print(lex.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "An essential method of LexStat is `get_scorer()`, which creates a scoring function based on sound correspondences. A number of different properties can be set to control the correspondence calculation, including:\n",
    "\n",
    "- `ratio`, which defines the ratio between derived and original score for sound matches\n",
    "- `vscale`, which defines a scaling factor for vowels, in order to decrease their score in the calculations\n",
    "- `threshold`, which defines the threshold used to select those words that are compared in order to derive the attested distribution\n",
    "- `unattested`, which allows to set the value for a pair of sounds which is expected by the alignment algorithm but which is not attesed in the data (whose score would otherwise be -infinity)\n",
    "\n",
    "Running `get_scorer()` should take a little time, as it calculates both the attested and the random alignments for all language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CORRESPONDENCE CALCULATION:   0%|          | 0/24.5 [00:00<?, ?it/s]2017-02-11 18:11:33,254 [INFO] Calculating alignments for pair Albanian / Albanian.\n",
      "CORRESPONDENCE CALCULATION:   8%|▊         | 2/24.5 [00:00<00:01, 19.72it/s]2017-02-11 18:11:33,363 [INFO] Calculating alignments for pair Albanian / English.\n",
      "CORRESPONDENCE CALCULATION:  12%|█▏        | 3/24.5 [00:00<00:01, 14.95it/s]2017-02-11 18:11:33,462 [INFO] Calculating alignments for pair Albanian / French.\n",
      "CORRESPONDENCE CALCULATION:  16%|█▋        | 4/24.5 [00:00<00:01, 12.76it/s]2017-02-11 18:11:33,568 [INFO] Calculating alignments for pair Albanian / German.\n",
      "CORRESPONDENCE CALCULATION:  20%|██        | 5/24.5 [00:00<00:01, 11.56it/s]2017-02-11 18:11:33,673 [INFO] Calculating alignments for pair Albanian / Hawaiian.\n",
      "CORRESPONDENCE CALCULATION:  24%|██▍       | 6/24.5 [00:00<00:01, 10.52it/s]2017-02-11 18:11:33,788 [INFO] Calculating alignments for pair Albanian / Navajo.\n",
      "CORRESPONDENCE CALCULATION:  29%|██▊       | 7/24.5 [00:00<00:01,  9.86it/s]2017-02-11 18:11:33,906 [INFO] Calculating alignments for pair Albanian / Turkish.\n",
      "CORRESPONDENCE CALCULATION:  33%|███▎      | 8/24.5 [00:00<00:01,  9.65it/s]2017-02-11 18:11:34,015 [INFO] Calculating alignments for pair English / English.\n",
      "CORRESPONDENCE CALCULATION:  37%|███▋      | 9/24.5 [00:00<00:01,  9.74it/s]2017-02-11 18:11:34,113 [INFO] Calculating alignments for pair English / French.\n",
      "CORRESPONDENCE CALCULATION:  41%|████      | 10/24.5 [00:00<00:01,  9.75it/s]2017-02-11 18:11:34,217 [INFO] Calculating alignments for pair English / German.\n",
      "CORRESPONDENCE CALCULATION:  45%|████▍     | 11/24.5 [00:01<00:01,  9.73it/s]2017-02-11 18:11:34,319 [INFO] Calculating alignments for pair English / Hawaiian.\n",
      "CORRESPONDENCE CALCULATION:  49%|████▉     | 12/24.5 [00:01<00:01,  9.48it/s]2017-02-11 18:11:34,431 [INFO] Calculating alignments for pair English / Navajo.\n",
      "CORRESPONDENCE CALCULATION:  53%|█████▎    | 13/24.5 [00:01<00:01,  9.31it/s]2017-02-11 18:11:34,543 [INFO] Calculating alignments for pair English / Turkish.\n",
      "CORRESPONDENCE CALCULATION:  57%|█████▋    | 14/24.5 [00:01<00:01,  9.42it/s]2017-02-11 18:11:34,646 [INFO] Calculating alignments for pair French / French.\n",
      "CORRESPONDENCE CALCULATION:  61%|██████    | 15/24.5 [00:01<00:00,  9.56it/s]2017-02-11 18:11:34,747 [INFO] Calculating alignments for pair French / German.\n",
      "CORRESPONDENCE CALCULATION:  65%|██████▌   | 16/24.5 [00:01<00:00,  9.58it/s]2017-02-11 18:11:34,851 [INFO] Calculating alignments for pair French / Hawaiian.\n",
      "CORRESPONDENCE CALCULATION:  69%|██████▉   | 17/24.5 [00:01<00:00,  9.33it/s]2017-02-11 18:11:34,964 [INFO] Calculating alignments for pair French / Navajo.\n",
      "CORRESPONDENCE CALCULATION:  73%|███████▎  | 18/24.5 [00:01<00:00,  9.00it/s]2017-02-11 18:11:35,085 [INFO] Calculating alignments for pair French / Turkish.\n",
      "CORRESPONDENCE CALCULATION:  78%|███████▊  | 19/24.5 [00:01<00:00,  9.03it/s]2017-02-11 18:11:35,195 [INFO] Calculating alignments for pair German / German.\n",
      "CORRESPONDENCE CALCULATION:  82%|████████▏ | 20/24.5 [00:02<00:00,  9.10it/s]2017-02-11 18:11:35,303 [INFO] Calculating alignments for pair German / Hawaiian.\n",
      "CORRESPONDENCE CALCULATION:  86%|████████▌ | 21/24.5 [00:02<00:00,  8.87it/s]2017-02-11 18:11:35,423 [INFO] Calculating alignments for pair German / Navajo.\n",
      "CORRESPONDENCE CALCULATION:  90%|████████▉ | 22/24.5 [00:02<00:00,  8.72it/s]2017-02-11 18:11:35,545 [INFO] Calculating alignments for pair German / Turkish.\n",
      "CORRESPONDENCE CALCULATION:  94%|█████████▍| 23/24.5 [00:02<00:00,  8.65it/s]2017-02-11 18:11:35,660 [INFO] Calculating alignments for pair Hawaiian / Hawaiian.\n",
      "CORRESPONDENCE CALCULATION:  98%|█████████▊| 24/24.5 [00:02<00:00,  8.26it/s]2017-02-11 18:11:35,793 [INFO] Calculating alignments for pair Hawaiian / Navajo.\n",
      "CORRESPONDENCE CALCULATION: 25it [00:02,  8.13it/s]                          2017-02-11 18:11:35,921 [INFO] Calculating alignments for pair Hawaiian / Turkish.\n",
      "CORRESPONDENCE CALCULATION: 26it [00:02,  8.17it/s]2017-02-11 18:11:36,041 [INFO] Calculating alignments for pair Navajo / Navajo.\n",
      "CORRESPONDENCE CALCULATION: 27it [00:02,  8.06it/s]2017-02-11 18:11:36,169 [INFO] Calculating alignments for pair Navajo / Turkish.\n",
      "CORRESPONDENCE CALCULATION: 28it [00:03,  8.13it/s]2017-02-11 18:11:36,290 [INFO] Calculating alignments for pair Turkish / Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION:   0%|          | 0/24.5 [00:00<?, ?it/s]2017-02-11 18:11:36,408 [INFO] Calculating random alignmentsfor pair Albanian/Albanian.\n",
      "RANDOM CORRESPONDENCE CALCULATION:   8%|▊         | 2/24.5 [00:00<00:06,  3.55it/s]2017-02-11 18:11:36,974 [INFO] Calculating random alignmentsfor pair Albanian/English.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  12%|█▏        | 3/24.5 [00:01<00:07,  2.98it/s]2017-02-11 18:11:37,434 [INFO] Calculating random alignmentsfor pair Albanian/French.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  16%|█▋        | 4/24.5 [00:01<00:07,  2.64it/s]2017-02-11 18:11:37,913 [INFO] Calculating random alignmentsfor pair Albanian/German.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  20%|██        | 5/24.5 [00:01<00:08,  2.43it/s]2017-02-11 18:11:38,404 [INFO] Calculating random alignmentsfor pair Albanian/Hawaiian.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  24%|██▍       | 6/24.5 [00:02<00:08,  2.20it/s]2017-02-11 18:11:38,959 [INFO] Calculating random alignmentsfor pair Albanian/Navajo.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  29%|██▊       | 7/24.5 [00:03<00:08,  2.08it/s]2017-02-11 18:11:39,500 [INFO] Calculating random alignmentsfor pair Albanian/Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  33%|███▎      | 8/24.5 [00:03<00:08,  2.02it/s]2017-02-11 18:11:40,026 [INFO] Calculating random alignmentsfor pair English/English.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  37%|███▋      | 9/24.5 [00:04<00:07,  2.06it/s]2017-02-11 18:11:40,491 [INFO] Calculating random alignmentsfor pair English/French.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  41%|████      | 10/24.5 [00:04<00:06,  2.10it/s]2017-02-11 18:11:40,946 [INFO] Calculating random alignmentsfor pair English/German.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  45%|████▍     | 11/24.5 [00:04<00:06,  2.12it/s]2017-02-11 18:11:41,408 [INFO] Calculating random alignmentsfor pair English/Hawaiian.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  49%|████▉     | 12/24.5 [00:05<00:06,  2.05it/s]2017-02-11 18:11:41,934 [INFO] Calculating random alignmentsfor pair English/Navajo.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  53%|█████▎    | 13/24.5 [00:06<00:05,  2.01it/s]2017-02-11 18:11:42,454 [INFO] Calculating random alignmentsfor pair English/Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  57%|█████▋    | 14/24.5 [00:06<00:05,  2.02it/s]2017-02-11 18:11:42,944 [INFO] Calculating random alignmentsfor pair French/French.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  61%|██████    | 15/24.5 [00:07<00:04,  2.03it/s]2017-02-11 18:11:43,432 [INFO] Calculating random alignmentsfor pair French/German.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  65%|██████▌   | 16/24.5 [00:07<00:04,  2.03it/s]2017-02-11 18:11:43,931 [INFO] Calculating random alignmentsfor pair French/Hawaiian.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  69%|██████▉   | 17/24.5 [00:08<00:03,  1.94it/s]2017-02-11 18:11:44,498 [INFO] Calculating random alignmentsfor pair French/Navajo.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  73%|███████▎  | 18/24.5 [00:08<00:03,  1.88it/s]2017-02-11 18:11:45,063 [INFO] Calculating random alignmentsfor pair French/Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  78%|███████▊  | 19/24.5 [00:09<00:02,  1.90it/s]2017-02-11 18:11:45,577 [INFO] Calculating random alignmentsfor pair German/German.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  82%|████████▏ | 20/24.5 [00:09<00:02,  1.94it/s]2017-02-11 18:11:46,070 [INFO] Calculating random alignmentsfor pair German/Hawaiian.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  86%|████████▌ | 21/24.5 [00:10<00:01,  1.81it/s]2017-02-11 18:11:46,710 [INFO] Calculating random alignmentsfor pair German/Navajo.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  90%|████████▉ | 22/24.5 [00:10<00:01,  1.81it/s]2017-02-11 18:11:47,264 [INFO] Calculating random alignmentsfor pair German/Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  94%|█████████▍| 23/24.5 [00:11<00:00,  1.85it/s]2017-02-11 18:11:47,774 [INFO] Calculating random alignmentsfor pair Hawaiian/Hawaiian.\n",
      "RANDOM CORRESPONDENCE CALCULATION:  98%|█████████▊| 24/24.5 [00:11<00:00,  1.78it/s]2017-02-11 18:11:48,383 [INFO] Calculating random alignmentsfor pair Hawaiian/Navajo.\n",
      "RANDOM CORRESPONDENCE CALCULATION: 25it [00:12,  1.75it/s]                          2017-02-11 18:11:48,983 [INFO] Calculating random alignmentsfor pair Hawaiian/Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION: 26it [00:13,  1.74it/s]2017-02-11 18:11:49,563 [INFO] Calculating random alignmentsfor pair Navajo/Navajo.\n",
      "RANDOM CORRESPONDENCE CALCULATION: 27it [00:13,  1.68it/s]2017-02-11 18:11:50,207 [INFO] Calculating random alignmentsfor pair Navajo/Turkish.\n",
      "RANDOM CORRESPONDENCE CALCULATION: 28it [00:14,  1.71it/s]2017-02-11 18:11:50,770 [INFO] Calculating random alignmentsfor pair Turkish/Turkish.\n",
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "lex.get_scorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once the calculation of the scoring function for our data is completed, we can run the methods for the analyses we desire, such as the one for flat clustering the words in the dataset to cognate sets (ie., to estimate the COGID).\n",
    "\n",
    "Given that we have a column with a golden standard for the COCID, we can analyze how well the automatic cognate detection is working with statistical methods such as the B-Cubed scores (Bagga1999). These rank between 0 and 1, with one being good and 0 being bad, and come in three flavors of precision (amount of false positives), recall (amount of false negatives) and F-Score (combined score). We compute them by passing the wordlist object, the gold standard (stored in column `\"cogid\"` in our file), and our computed cognate sets (stored in the column we specify using the `\"ref\"` keyword). Let's experiment with different methods and thresholds (please note that this computation can take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SEQUENCE CLUSTERING:   0%|          | 0/200 [00:00<?, ?it/s]2017-02-11 18:11:51,852 [INFO] Analyzing words for concept <I>.\n",
      "2017-02-11 18:11:51,861 [INFO] Analyzing words for concept <all>.\n",
      "2017-02-11 18:11:51,872 [INFO] Analyzing words for concept <and>.\n",
      "2017-02-11 18:11:51,884 [INFO] Analyzing words for concept <animal>.\n",
      "2017-02-11 18:11:51,900 [INFO] Analyzing words for concept <ashes>.\n",
      "2017-02-11 18:11:51,909 [INFO] Analyzing words for concept <at>.\n",
      "2017-02-11 18:11:51,920 [INFO] Analyzing words for concept <back>.\n",
      "2017-02-11 18:11:51,931 [INFO] Analyzing words for concept <bad>.\n",
      "2017-02-11 18:11:51,944 [INFO] Analyzing words for concept <bark>.\n",
      "SEQUENCE CLUSTERING:   4%|▍         | 9/200 [00:00<00:02, 85.80it/s]2017-02-11 18:11:51,961 [INFO] Analyzing words for concept <because>.\n",
      "2017-02-11 18:11:51,973 [INFO] Analyzing words for concept <belly>.\n",
      "2017-02-11 18:11:51,984 [INFO] Analyzing words for concept <big>.\n",
      "2017-02-11 18:11:51,995 [INFO] Analyzing words for concept <bird>.\n",
      "2017-02-11 18:11:52,007 [INFO] Analyzing words for concept <bite>.\n",
      "2017-02-11 18:11:52,019 [INFO] Analyzing words for concept <black>.\n",
      "2017-02-11 18:11:52,033 [INFO] Analyzing words for concept <blood>.\n",
      "2017-02-11 18:11:52,044 [INFO] Analyzing words for concept <blow>.\n",
      "2017-02-11 18:11:52,054 [INFO] Analyzing words for concept <bone>.\n",
      "SEQUENCE CLUSTERING:   9%|▉         | 18/200 [00:00<00:02, 85.34it/s]2017-02-11 18:11:52,067 [INFO] Analyzing words for concept <breast>.\n",
      "2017-02-11 18:11:52,076 [INFO] Analyzing words for concept <breathe>.\n",
      "2017-02-11 18:11:52,089 [INFO] Analyzing words for concept <burn>.\n",
      "2017-02-11 18:11:52,099 [INFO] Analyzing words for concept <child>.\n",
      "2017-02-11 18:11:52,110 [INFO] Analyzing words for concept <claw>.\n",
      "2017-02-11 18:11:52,122 [INFO] Analyzing words for concept <cloud>.\n",
      "2017-02-11 18:11:52,133 [INFO] Analyzing words for concept <cold>.\n",
      "2017-02-11 18:11:52,146 [INFO] Analyzing words for concept <come>.\n",
      "2017-02-11 18:11:52,156 [INFO] Analyzing words for concept <count>.\n",
      "SEQUENCE CLUSTERING:  14%|█▎        | 27/200 [00:00<00:02, 85.80it/s]2017-02-11 18:11:52,170 [INFO] Analyzing words for concept <cut>.\n",
      "2017-02-11 18:11:52,181 [INFO] Analyzing words for concept <day>.\n",
      "2017-02-11 18:11:52,190 [INFO] Analyzing words for concept <die>.\n",
      "2017-02-11 18:11:52,201 [INFO] Analyzing words for concept <dig>.\n",
      "2017-02-11 18:11:52,213 [INFO] Analyzing words for concept <dirty>.\n",
      "2017-02-11 18:11:52,224 [INFO] Analyzing words for concept <dog>.\n",
      "2017-02-11 18:11:52,236 [INFO] Analyzing words for concept <drink>.\n",
      "2017-02-11 18:11:52,247 [INFO] Analyzing words for concept <dry>.\n",
      "SEQUENCE CLUSTERING:  18%|█▊        | 35/200 [00:00<00:01, 83.78it/s]2017-02-11 18:11:52,272 [INFO] Analyzing words for concept <dull>.\n",
      "2017-02-11 18:11:52,287 [INFO] Analyzing words for concept <dust>.\n",
      "2017-02-11 18:11:52,305 [INFO] Analyzing words for concept <ear>.\n",
      "2017-02-11 18:11:52,315 [INFO] Analyzing words for concept <earth>.\n",
      "2017-02-11 18:11:52,327 [INFO] Analyzing words for concept <eat>.\n",
      "2017-02-11 18:11:52,338 [INFO] Analyzing words for concept <egg>.\n",
      "2017-02-11 18:11:52,349 [INFO] Analyzing words for concept <eye>.\n",
      "2017-02-11 18:11:52,359 [INFO] Analyzing words for concept <fall>.\n",
      "SEQUENCE CLUSTERING:  22%|██▏       | 43/200 [00:00<00:01, 82.33it/s]2017-02-11 18:11:52,375 [INFO] Analyzing words for concept <far>.\n",
      "2017-02-11 18:11:52,385 [INFO] Analyzing words for concept <father>.\n",
      "2017-02-11 18:11:52,398 [INFO] Analyzing words for concept <feather>.\n",
      "2017-02-11 18:11:52,410 [INFO] Analyzing words for concept <few>.\n",
      "2017-02-11 18:11:52,423 [INFO] Analyzing words for concept <fight>.\n",
      "2017-02-11 18:11:52,438 [INFO] Analyzing words for concept <fire>.\n",
      "2017-02-11 18:11:52,448 [INFO] Analyzing words for concept <fish>.\n",
      "2017-02-11 18:11:52,460 [INFO] Analyzing words for concept <five>.\n",
      "SEQUENCE CLUSTERING:  26%|██▌       | 51/200 [00:00<00:01, 80.68it/s]2017-02-11 18:11:52,477 [INFO] Analyzing words for concept <flow>.\n",
      "2017-02-11 18:11:52,489 [INFO] Analyzing words for concept <flower>.\n",
      "2017-02-11 18:11:52,502 [INFO] Analyzing words for concept <fly>.\n",
      "2017-02-11 18:11:52,518 [INFO] Analyzing words for concept <fog>.\n",
      "2017-02-11 18:11:52,529 [INFO] Analyzing words for concept <foot>.\n",
      "2017-02-11 18:11:52,541 [INFO] Analyzing words for concept <four>.\n",
      "2017-02-11 18:11:52,552 [INFO] Analyzing words for concept <freeze>.\n",
      "2017-02-11 18:11:52,565 [INFO] Analyzing words for concept <fruit>.\n",
      "SEQUENCE CLUSTERING:  30%|██▉       | 59/200 [00:00<00:01, 79.27it/s]2017-02-11 18:11:52,583 [INFO] Analyzing words for concept <full>.\n",
      "2017-02-11 18:11:52,597 [INFO] Analyzing words for concept <give>.\n",
      "2017-02-11 18:11:52,609 [INFO] Analyzing words for concept <go>.\n",
      "2017-02-11 18:11:52,620 [INFO] Analyzing words for concept <good>.\n",
      "2017-02-11 18:11:52,632 [INFO] Analyzing words for concept <grass>.\n",
      "2017-02-11 18:11:52,643 [INFO] Analyzing words for concept <grease>.\n",
      "2017-02-11 18:11:52,655 [INFO] Analyzing words for concept <green>.\n",
      "2017-02-11 18:11:52,670 [INFO] Analyzing words for concept <guts>.\n",
      "SEQUENCE CLUSTERING:  34%|███▎      | 67/200 [00:00<00:01, 78.57it/s]2017-02-11 18:11:52,685 [INFO] Analyzing words for concept <hair>.\n",
      "2017-02-11 18:11:52,697 [INFO] Analyzing words for concept <hand>.\n",
      "2017-02-11 18:11:52,709 [INFO] Analyzing words for concept <he>.\n",
      "2017-02-11 18:11:52,718 [INFO] Analyzing words for concept <head>.\n",
      "2017-02-11 18:11:52,729 [INFO] Analyzing words for concept <hear>.\n",
      "2017-02-11 18:11:52,741 [INFO] Analyzing words for concept <heart>.\n",
      "2017-02-11 18:11:52,757 [INFO] Analyzing words for concept <heavy>.\n",
      "2017-02-11 18:11:52,770 [INFO] Analyzing words for concept <here>.\n",
      "2017-02-11 18:11:52,781 [INFO] Analyzing words for concept <hit>.\n",
      "SEQUENCE CLUSTERING:  38%|███▊      | 76/200 [00:00<00:01, 79.20it/s]2017-02-11 18:11:52,797 [INFO] Analyzing words for concept <hold>.\n",
      "2017-02-11 18:11:52,810 [INFO] Analyzing words for concept <horn>.\n",
      "2017-02-11 18:11:52,821 [INFO] Analyzing words for concept <hot>.\n",
      "2017-02-11 18:11:52,835 [INFO] Analyzing words for concept <human>.\n",
      "2017-02-11 18:11:52,849 [INFO] Analyzing words for concept <hunt>.\n",
      "2017-02-11 18:11:52,860 [INFO] Analyzing words for concept <husband>.\n",
      "2017-02-11 18:11:52,874 [INFO] Analyzing words for concept <ice>.\n",
      "2017-02-11 18:11:52,885 [INFO] Analyzing words for concept <if>.\n",
      "SEQUENCE CLUSTERING:  42%|████▏     | 84/200 [00:01<00:01, 78.42it/s]2017-02-11 18:11:52,901 [INFO] Analyzing words for concept <in>.\n",
      "2017-02-11 18:11:52,912 [INFO] Analyzing words for concept <kill>.\n",
      "2017-02-11 18:11:52,925 [INFO] Analyzing words for concept <knee>.\n",
      "2017-02-11 18:11:52,936 [INFO] Analyzing words for concept <knife>.\n",
      "2017-02-11 18:11:52,948 [INFO] Analyzing words for concept <know>.\n",
      "2017-02-11 18:11:52,958 [INFO] Analyzing words for concept <lake>.\n",
      "2017-02-11 18:11:52,970 [INFO] Analyzing words for concept <laugh>.\n",
      "2017-02-11 18:11:52,980 [INFO] Analyzing words for concept <leaf>.\n",
      "2017-02-11 18:11:52,994 [INFO] Analyzing words for concept <left>.\n",
      "SEQUENCE CLUSTERING:  46%|████▋     | 93/200 [00:01<00:01, 79.86it/s]2017-02-11 18:11:53,011 [INFO] Analyzing words for concept <lie>.\n",
      "2017-02-11 18:11:53,023 [INFO] Analyzing words for concept <liver>.\n",
      "2017-02-11 18:11:53,036 [INFO] Analyzing words for concept <long>.\n",
      "2017-02-11 18:11:53,048 [INFO] Analyzing words for concept <louse>.\n",
      "2017-02-11 18:11:53,060 [INFO] Analyzing words for concept <man>.\n",
      "2017-02-11 18:11:53,071 [INFO] Analyzing words for concept <many>.\n",
      "2017-02-11 18:11:53,082 [INFO] Analyzing words for concept <meat>.\n",
      "2017-02-11 18:11:53,094 [INFO] Analyzing words for concept <moon>.\n",
      "SEQUENCE CLUSTERING:  50%|█████     | 101/200 [00:01<00:01, 79.88it/s]2017-02-11 18:11:53,110 [INFO] Analyzing words for concept <mother>.\n",
      "2017-02-11 18:11:53,126 [INFO] Analyzing words for concept <mountain>.\n",
      "2017-02-11 18:11:53,137 [INFO] Analyzing words for concept <mouth>.\n",
      "2017-02-11 18:11:53,148 [INFO] Analyzing words for concept <name>.\n",
      "2017-02-11 18:11:53,161 [INFO] Analyzing words for concept <narrow>.\n",
      "2017-02-11 18:11:53,175 [INFO] Analyzing words for concept <near>.\n",
      "2017-02-11 18:11:53,189 [INFO] Analyzing words for concept <neck>.\n",
      "2017-02-11 18:11:53,201 [INFO] Analyzing words for concept <new>.\n",
      "SEQUENCE CLUSTERING:  55%|█████▍    | 109/200 [00:01<00:01, 78.47it/s]2017-02-11 18:11:53,218 [INFO] Analyzing words for concept <night>.\n",
      "2017-02-11 18:11:53,227 [INFO] Analyzing words for concept <nose>.\n",
      "2017-02-11 18:11:53,246 [INFO] Analyzing words for concept <not>.\n",
      "2017-02-11 18:11:53,259 [INFO] Analyzing words for concept <now>.\n",
      "2017-02-11 18:11:53,270 [INFO] Analyzing words for concept <old>.\n",
      "2017-02-11 18:11:53,283 [INFO] Analyzing words for concept <one>.\n",
      "2017-02-11 18:11:53,296 [INFO] Analyzing words for concept <other>.\n",
      "2017-02-11 18:11:53,309 [INFO] Analyzing words for concept <path>.\n",
      "SEQUENCE CLUSTERING:  58%|█████▊    | 117/200 [00:01<00:01, 77.57it/s]2017-02-11 18:11:53,322 [INFO] Analyzing words for concept <play>.\n",
      "2017-02-11 18:11:53,335 [INFO] Analyzing words for concept <pull>.\n",
      "2017-02-11 18:11:53,347 [INFO] Analyzing words for concept <push>.\n",
      "2017-02-11 18:11:53,360 [INFO] Analyzing words for concept <rain>.\n",
      "2017-02-11 18:11:53,372 [INFO] Analyzing words for concept <red>.\n",
      "2017-02-11 18:11:53,384 [INFO] Analyzing words for concept <right>.\n",
      "2017-02-11 18:11:53,397 [INFO] Analyzing words for concept <river>.\n",
      "2017-02-11 18:11:53,411 [INFO] Analyzing words for concept <root>.\n",
      "SEQUENCE CLUSTERING:  62%|██████▎   | 125/200 [00:01<00:00, 77.17it/s]2017-02-11 18:11:53,427 [INFO] Analyzing words for concept <rotten>.\n",
      "2017-02-11 18:11:53,441 [INFO] Analyzing words for concept <round>.\n",
      "2017-02-11 18:11:53,457 [INFO] Analyzing words for concept <rub>.\n",
      "2017-02-11 18:11:53,473 [INFO] Analyzing words for concept <salt>.\n",
      "2017-02-11 18:11:53,484 [INFO] Analyzing words for concept <sand>.\n",
      "2017-02-11 18:11:53,497 [INFO] Analyzing words for concept <say>.\n",
      "2017-02-11 18:11:53,509 [INFO] Analyzing words for concept <scratch>.\n",
      "2017-02-11 18:11:53,522 [INFO] Analyzing words for concept <sea>.\n",
      "SEQUENCE CLUSTERING:  66%|██████▋   | 133/200 [00:01<00:00, 75.76it/s]2017-02-11 18:11:53,537 [INFO] Analyzing words for concept <see>.\n",
      "2017-02-11 18:11:53,548 [INFO] Analyzing words for concept <seed>.\n",
      "2017-02-11 18:11:53,562 [INFO] Analyzing words for concept <sew>.\n",
      "2017-02-11 18:11:53,573 [INFO] Analyzing words for concept <sharp>.\n",
      "2017-02-11 18:11:53,587 [INFO] Analyzing words for concept <short>.\n",
      "2017-02-11 18:11:53,601 [INFO] Analyzing words for concept <sing>.\n",
      "2017-02-11 18:11:53,622 [INFO] Analyzing words for concept <sit>.\n",
      "2017-02-11 18:11:53,633 [INFO] Analyzing words for concept <skin>.\n",
      "SEQUENCE CLUSTERING:  70%|███████   | 141/200 [00:01<00:00, 74.08it/s]2017-02-11 18:11:53,652 [INFO] Analyzing words for concept <sky>.\n",
      "2017-02-11 18:11:53,665 [INFO] Analyzing words for concept <sleep>.\n",
      "2017-02-11 18:11:53,677 [INFO] Analyzing words for concept <small>.\n",
      "2017-02-11 18:11:53,690 [INFO] Analyzing words for concept <smell>.\n",
      "2017-02-11 18:11:53,703 [INFO] Analyzing words for concept <smoke>.\n",
      "2017-02-11 18:11:53,715 [INFO] Analyzing words for concept <smooth>.\n",
      "2017-02-11 18:11:53,729 [INFO] Analyzing words for concept <snake>.\n",
      "2017-02-11 18:11:53,744 [INFO] Analyzing words for concept <snow>.\n",
      "SEQUENCE CLUSTERING:  74%|███████▍  | 149/200 [00:01<00:00, 73.39it/s]2017-02-11 18:11:53,763 [INFO] Analyzing words for concept <some>.\n",
      "2017-02-11 18:11:53,778 [INFO] Analyzing words for concept <spit>.\n",
      "2017-02-11 18:11:53,794 [INFO] Analyzing words for concept <split>.\n",
      "2017-02-11 18:11:53,809 [INFO] Analyzing words for concept <squeeze>.\n",
      "2017-02-11 18:11:53,827 [INFO] Analyzing words for concept <stab>.\n",
      "2017-02-11 18:11:53,844 [INFO] Analyzing words for concept <stand>.\n",
      "2017-02-11 18:11:53,855 [INFO] Analyzing words for concept <star>.\n",
      "2017-02-11 18:11:53,868 [INFO] Analyzing words for concept <stick>.\n",
      "SEQUENCE CLUSTERING:  78%|███████▊  | 157/200 [00:02<00:00, 70.84it/s]2017-02-11 18:11:53,884 [INFO] Analyzing words for concept <stone>.\n",
      "2017-02-11 18:11:53,897 [INFO] Analyzing words for concept <straight>.\n",
      "2017-02-11 18:11:53,914 [INFO] Analyzing words for concept <suck>.\n",
      "2017-02-11 18:11:53,928 [INFO] Analyzing words for concept <sun>.\n",
      "2017-02-11 18:11:53,937 [INFO] Analyzing words for concept <swell>.\n",
      "2017-02-11 18:11:53,951 [INFO] Analyzing words for concept <swim>.\n",
      "2017-02-11 18:11:53,964 [INFO] Analyzing words for concept <tail>.\n",
      "2017-02-11 18:11:53,979 [INFO] Analyzing words for concept <that>.\n",
      "SEQUENCE CLUSTERING:  82%|████████▎ | 165/200 [00:02<00:00, 71.78it/s]2017-02-11 18:11:53,993 [INFO] Analyzing words for concept <there>.\n",
      "2017-02-11 18:11:54,004 [INFO] Analyzing words for concept <they>.\n",
      "2017-02-11 18:11:54,015 [INFO] Analyzing words for concept <thick>.\n",
      "2017-02-11 18:11:54,027 [INFO] Analyzing words for concept <thin>.\n",
      "2017-02-11 18:11:54,040 [INFO] Analyzing words for concept <think>.\n",
      "2017-02-11 18:11:54,054 [INFO] Analyzing words for concept <this>.\n",
      "2017-02-11 18:11:54,066 [INFO] Analyzing words for concept <thou>.\n",
      "2017-02-11 18:11:54,076 [INFO] Analyzing words for concept <three>.\n",
      "2017-02-11 18:11:54,088 [INFO] Analyzing words for concept <throw>.\n",
      "SEQUENCE CLUSTERING:  87%|████████▋ | 174/200 [00:02<00:00, 74.41it/s]2017-02-11 18:11:54,104 [INFO] Analyzing words for concept <tie>.\n",
      "2017-02-11 18:11:54,115 [INFO] Analyzing words for concept <tongue>.\n",
      "2017-02-11 18:11:54,128 [INFO] Analyzing words for concept <tooth>.\n",
      "2017-02-11 18:11:54,140 [INFO] Analyzing words for concept <tree>.\n",
      "2017-02-11 18:11:54,152 [INFO] Analyzing words for concept <true>.\n",
      "2017-02-11 18:11:54,165 [INFO] Analyzing words for concept <two>.\n",
      "2017-02-11 18:11:54,180 [INFO] Analyzing words for concept <vomit>.\n",
      "2017-02-11 18:11:54,194 [INFO] Analyzing words for concept <wash>.\n",
      "SEQUENCE CLUSTERING:  91%|█████████ | 182/200 [00:02<00:00, 74.36it/s]2017-02-11 18:11:54,212 [INFO] Analyzing words for concept <water>.\n",
      "2017-02-11 18:11:54,223 [INFO] Analyzing words for concept <we>.\n",
      "2017-02-11 18:11:54,234 [INFO] Analyzing words for concept <wet>.\n",
      "2017-02-11 18:11:54,247 [INFO] Analyzing words for concept <what>.\n",
      "2017-02-11 18:11:54,257 [INFO] Analyzing words for concept <white>.\n",
      "2017-02-11 18:11:54,271 [INFO] Analyzing words for concept <who>.\n",
      "2017-02-11 18:11:54,281 [INFO] Analyzing words for concept <wide>.\n",
      "2017-02-11 18:11:54,293 [INFO] Analyzing words for concept <wife>.\n",
      "2017-02-11 18:11:54,306 [INFO] Analyzing words for concept <wind>.\n",
      "SEQUENCE CLUSTERING:  96%|█████████▌| 191/200 [00:02<00:00, 76.19it/s]2017-02-11 18:11:54,322 [INFO] Analyzing words for concept <wing>.\n",
      "2017-02-11 18:11:54,334 [INFO] Analyzing words for concept <wipe>.\n",
      "2017-02-11 18:11:54,348 [INFO] Analyzing words for concept <with>.\n",
      "2017-02-11 18:11:54,358 [INFO] Analyzing words for concept <woman>.\n",
      "2017-02-11 18:11:54,372 [INFO] Analyzing words for concept <woods>.\n",
      "2017-02-11 18:11:54,385 [INFO] Analyzing words for concept <worm>.\n",
      "2017-02-11 18:11:54,398 [INFO] Analyzing words for concept <year>.\n",
      "2017-02-11 18:11:54,411 [INFO] Analyzing words for concept <yellow>.\n",
      "SEQUENCE CLUSTERING: 100%|█████████▉| 199/200 [00:02<00:00, 76.11it/s]2017-02-11 18:11:54,428 [INFO] Analyzing words for concept <you>.\n",
      "SEQUENCE CLUSTERING:   0%|          | 0/200 [00:00<?, ?it/s]          2017-02-11 18:11:54,450 [INFO] Analyzing words for concept <I>.\n",
      "2017-02-11 18:11:54,463 [INFO] Analyzing words for concept <all>.\n",
      "2017-02-11 18:11:54,478 [INFO] Analyzing words for concept <and>.\n",
      "2017-02-11 18:11:54,490 [INFO] Analyzing words for concept <animal>.\n",
      "2017-02-11 18:11:54,513 [INFO] Analyzing words for concept <ashes>.\n",
      "2017-02-11 18:11:54,529 [INFO] Analyzing words for concept <at>.\n",
      "2017-02-11 18:11:54,543 [INFO] Analyzing words for concept <back>.\n",
      "SEQUENCE CLUSTERING:   4%|▎         | 7/200 [00:00<00:03, 64.16it/s]2017-02-11 18:11:54,566 [INFO] Analyzing words for concept <bad>.\n",
      "2017-02-11 18:11:54,585 [INFO] Analyzing words for concept <bark>.\n",
      "2017-02-11 18:11:54,612 [INFO] Analyzing words for concept <because>.\n",
      "2017-02-11 18:11:54,629 [INFO] Analyzing words for concept <belly>.\n",
      "2017-02-11 18:11:54,647 [INFO] Analyzing words for concept <big>.\n",
      "SEQUENCE CLUSTERING:   6%|▌         | 12/200 [00:00<00:03, 56.94it/s]2017-02-11 18:11:54,673 [INFO] Analyzing words for concept <bird>.\n",
      "2017-02-11 18:11:54,689 [INFO] Analyzing words for concept <bite>.\n",
      "2017-02-11 18:11:54,707 [INFO] Analyzing words for concept <black>.\n",
      "2017-02-11 18:11:54,725 [INFO] Analyzing words for concept <blood>.\n",
      "2017-02-11 18:11:54,741 [INFO] Analyzing words for concept <blow>.\n",
      "2017-02-11 18:11:54,756 [INFO] Analyzing words for concept <bone>.\n",
      "SEQUENCE CLUSTERING:   9%|▉         | 18/200 [00:00<00:03, 57.54it/s]2017-02-11 18:11:54,776 [INFO] Analyzing words for concept <breast>.\n",
      "2017-02-11 18:11:54,791 [INFO] Analyzing words for concept <breathe>.\n",
      "2017-02-11 18:11:54,810 [INFO] Analyzing words for concept <burn>.\n",
      "2017-02-11 18:11:54,831 [INFO] Analyzing words for concept <child>.\n",
      "2017-02-11 18:11:54,847 [INFO] Analyzing words for concept <claw>.\n",
      "2017-02-11 18:11:54,866 [INFO] Analyzing words for concept <cloud>.\n",
      "SEQUENCE CLUSTERING:  12%|█▏        | 24/200 [00:00<00:03, 56.69it/s]2017-02-11 18:11:54,884 [INFO] Analyzing words for concept <cold>.\n",
      "2017-02-11 18:11:54,903 [INFO] Analyzing words for concept <come>.\n",
      "2017-02-11 18:11:54,919 [INFO] Analyzing words for concept <count>.\n",
      "2017-02-11 18:11:54,938 [INFO] Analyzing words for concept <cut>.\n",
      "2017-02-11 18:11:54,955 [INFO] Analyzing words for concept <day>.\n",
      "2017-02-11 18:11:54,968 [INFO] Analyzing words for concept <die>.\n",
      "SEQUENCE CLUSTERING:  15%|█▌        | 30/200 [00:00<00:02, 57.33it/s]2017-02-11 18:11:54,986 [INFO] Analyzing words for concept <dig>.\n",
      "2017-02-11 18:11:55,004 [INFO] Analyzing words for concept <dirty>.\n",
      "2017-02-11 18:11:55,022 [INFO] Analyzing words for concept <dog>.\n",
      "2017-02-11 18:11:55,040 [INFO] Analyzing words for concept <drink>.\n",
      "2017-02-11 18:11:55,056 [INFO] Analyzing words for concept <dry>.\n",
      "2017-02-11 18:11:55,074 [INFO] Analyzing words for concept <dull>.\n",
      "SEQUENCE CLUSTERING:  18%|█▊        | 36/200 [00:00<00:02, 56.62it/s]2017-02-11 18:11:55,096 [INFO] Analyzing words for concept <dust>.\n",
      "2017-02-11 18:11:55,114 [INFO] Analyzing words for concept <ear>.\n",
      "2017-02-11 18:11:55,130 [INFO] Analyzing words for concept <earth>.\n",
      "2017-02-11 18:11:55,146 [INFO] Analyzing words for concept <eat>.\n",
      "2017-02-11 18:11:55,159 [INFO] Analyzing words for concept <egg>.\n",
      "2017-02-11 18:11:55,174 [INFO] Analyzing words for concept <eye>.\n",
      "2017-02-11 18:11:55,188 [INFO] Analyzing words for concept <fall>.\n",
      "SEQUENCE CLUSTERING:  22%|██▏       | 43/200 [00:00<00:02, 58.41it/s]2017-02-11 18:11:55,206 [INFO] Analyzing words for concept <far>.\n",
      "2017-02-11 18:11:55,224 [INFO] Analyzing words for concept <father>.\n",
      "2017-02-11 18:11:55,243 [INFO] Analyzing words for concept <feather>.\n",
      "2017-02-11 18:11:55,260 [INFO] Analyzing words for concept <few>.\n",
      "2017-02-11 18:11:55,278 [INFO] Analyzing words for concept <fight>.\n",
      "2017-02-11 18:11:55,300 [INFO] Analyzing words for concept <fire>.\n",
      "SEQUENCE CLUSTERING:  24%|██▍       | 49/200 [00:00<00:02, 56.93it/s]2017-02-11 18:11:55,318 [INFO] Analyzing words for concept <fish>.\n",
      "2017-02-11 18:11:55,336 [INFO] Analyzing words for concept <five>.\n",
      "2017-02-11 18:11:55,353 [INFO] Analyzing words for concept <flow>.\n",
      "2017-02-11 18:11:55,369 [INFO] Analyzing words for concept <flower>.\n",
      "2017-02-11 18:11:55,387 [INFO] Analyzing words for concept <fly>.\n",
      "2017-02-11 18:11:55,405 [INFO] Analyzing words for concept <fog>.\n",
      "SEQUENCE CLUSTERING:  28%|██▊       | 55/200 [00:00<00:02, 56.55it/s]2017-02-11 18:11:55,427 [INFO] Analyzing words for concept <foot>.\n",
      "2017-02-11 18:11:55,442 [INFO] Analyzing words for concept <four>.\n",
      "2017-02-11 18:11:55,458 [INFO] Analyzing words for concept <freeze>.\n",
      "2017-02-11 18:11:55,477 [INFO] Analyzing words for concept <fruit>.\n",
      "2017-02-11 18:11:55,497 [INFO] Analyzing words for concept <full>.\n",
      "2017-02-11 18:11:55,514 [INFO] Analyzing words for concept <give>.\n",
      "SEQUENCE CLUSTERING:  30%|███       | 61/200 [00:01<00:02, 56.27it/s]2017-02-11 18:11:55,534 [INFO] Analyzing words for concept <go>.\n",
      "2017-02-11 18:11:55,549 [INFO] Analyzing words for concept <good>.\n",
      "2017-02-11 18:11:55,566 [INFO] Analyzing words for concept <grass>.\n",
      "2017-02-11 18:11:55,580 [INFO] Analyzing words for concept <grease>.\n",
      "2017-02-11 18:11:55,596 [INFO] Analyzing words for concept <green>.\n",
      "2017-02-11 18:11:55,616 [INFO] Analyzing words for concept <guts>.\n",
      "SEQUENCE CLUSTERING:  34%|███▎      | 67/200 [00:01<00:02, 56.90it/s]2017-02-11 18:11:55,637 [INFO] Analyzing words for concept <hair>.\n",
      "2017-02-11 18:11:55,654 [INFO] Analyzing words for concept <hand>.\n",
      "2017-02-11 18:11:55,669 [INFO] Analyzing words for concept <he>.\n",
      "2017-02-11 18:11:55,680 [INFO] Analyzing words for concept <head>.\n",
      "2017-02-11 18:11:55,695 [INFO] Analyzing words for concept <hear>.\n",
      "2017-02-11 18:11:55,714 [INFO] Analyzing words for concept <heart>.\n",
      "SEQUENCE CLUSTERING:  36%|███▋      | 73/200 [00:01<00:02, 57.33it/s]2017-02-11 18:11:55,740 [INFO] Analyzing words for concept <heavy>.\n",
      "2017-02-11 18:11:55,758 [INFO] Analyzing words for concept <here>.\n",
      "2017-02-11 18:11:55,774 [INFO] Analyzing words for concept <hit>.\n",
      "2017-02-11 18:11:55,792 [INFO] Analyzing words for concept <hold>.\n",
      "2017-02-11 18:11:55,809 [INFO] Analyzing words for concept <horn>.\n",
      "2017-02-11 18:11:55,829 [INFO] Analyzing words for concept <hot>.\n",
      "SEQUENCE CLUSTERING:  40%|███▉      | 79/200 [00:01<00:02, 56.49it/s]2017-02-11 18:11:55,853 [INFO] Analyzing words for concept <human>.\n",
      "2017-02-11 18:11:55,873 [INFO] Analyzing words for concept <hunt>.\n",
      "2017-02-11 18:11:55,890 [INFO] Analyzing words for concept <husband>.\n",
      "2017-02-11 18:11:55,909 [INFO] Analyzing words for concept <ice>.\n",
      "2017-02-11 18:11:55,924 [INFO] Analyzing words for concept <if>.\n",
      "2017-02-11 18:11:55,938 [INFO] Analyzing words for concept <in>.\n",
      "SEQUENCE CLUSTERING:  42%|████▎     | 85/200 [00:01<00:02, 56.71it/s]2017-02-11 18:11:55,955 [INFO] Analyzing words for concept <kill>.\n",
      "2017-02-11 18:11:55,977 [INFO] Analyzing words for concept <knee>.\n",
      "2017-02-11 18:11:55,991 [INFO] Analyzing words for concept <knife>.\n",
      "2017-02-11 18:11:56,007 [INFO] Analyzing words for concept <know>.\n",
      "2017-02-11 18:11:56,022 [INFO] Analyzing words for concept <lake>.\n",
      "2017-02-11 18:11:56,037 [INFO] Analyzing words for concept <laugh>.\n",
      "SEQUENCE CLUSTERING:  46%|████▌     | 91/200 [00:01<00:01, 57.53it/s]2017-02-11 18:11:56,056 [INFO] Analyzing words for concept <leaf>.\n",
      "2017-02-11 18:11:56,071 [INFO] Analyzing words for concept <left>.\n",
      "2017-02-11 18:11:56,090 [INFO] Analyzing words for concept <lie>.\n",
      "2017-02-11 18:11:56,104 [INFO] Analyzing words for concept <liver>.\n",
      "2017-02-11 18:11:56,121 [INFO] Analyzing words for concept <long>.\n",
      "2017-02-11 18:11:56,137 [INFO] Analyzing words for concept <louse>.\n",
      "2017-02-11 18:11:56,151 [INFO] Analyzing words for concept <man>.\n",
      "SEQUENCE CLUSTERING:  49%|████▉     | 98/200 [00:01<00:01, 58.32it/s]2017-02-11 18:11:56,171 [INFO] Analyzing words for concept <many>.\n",
      "2017-02-11 18:11:56,186 [INFO] Analyzing words for concept <meat>.\n",
      "2017-02-11 18:11:56,201 [INFO] Analyzing words for concept <moon>.\n",
      "2017-02-11 18:11:56,218 [INFO] Analyzing words for concept <mother>.\n",
      "2017-02-11 18:11:56,236 [INFO] Analyzing words for concept <mountain>.\n",
      "2017-02-11 18:11:56,254 [INFO] Analyzing words for concept <mouth>.\n",
      "SEQUENCE CLUSTERING:  52%|█████▏    | 104/200 [00:01<00:01, 58.67it/s]2017-02-11 18:11:56,272 [INFO] Analyzing words for concept <name>.\n",
      "2017-02-11 18:11:56,288 [INFO] Analyzing words for concept <narrow>.\n",
      "2017-02-11 18:11:56,309 [INFO] Analyzing words for concept <near>.\n",
      "2017-02-11 18:11:56,325 [INFO] Analyzing words for concept <neck>.\n",
      "2017-02-11 18:11:56,340 [INFO] Analyzing words for concept <new>.\n",
      "2017-02-11 18:11:56,355 [INFO] Analyzing words for concept <night>.\n",
      "SEQUENCE CLUSTERING:  55%|█████▌    | 110/200 [00:01<00:01, 58.97it/s]2017-02-11 18:11:56,373 [INFO] Analyzing words for concept <nose>.\n",
      "2017-02-11 18:11:56,388 [INFO] Analyzing words for concept <not>.\n",
      "2017-02-11 18:11:56,405 [INFO] Analyzing words for concept <now>.\n",
      "2017-02-11 18:11:56,421 [INFO] Analyzing words for concept <old>.\n",
      "2017-02-11 18:11:56,439 [INFO] Analyzing words for concept <one>.\n",
      "2017-02-11 18:11:56,454 [INFO] Analyzing words for concept <other>.\n",
      "SEQUENCE CLUSTERING:  58%|█████▊    | 116/200 [00:02<00:01, 59.25it/s]2017-02-11 18:11:56,473 [INFO] Analyzing words for concept <path>.\n",
      "2017-02-11 18:11:56,486 [INFO] Analyzing words for concept <play>.\n",
      "2017-02-11 18:11:56,503 [INFO] Analyzing words for concept <pull>.\n",
      "2017-02-11 18:11:56,519 [INFO] Analyzing words for concept <push>.\n",
      "2017-02-11 18:11:56,540 [INFO] Analyzing words for concept <rain>.\n",
      "2017-02-11 18:11:56,561 [INFO] Analyzing words for concept <red>.\n",
      "SEQUENCE CLUSTERING:  61%|██████    | 122/200 [00:02<00:01, 57.65it/s]2017-02-11 18:11:56,585 [INFO] Analyzing words for concept <right>.\n",
      "2017-02-11 18:11:56,603 [INFO] Analyzing words for concept <river>.\n",
      "2017-02-11 18:11:56,622 [INFO] Analyzing words for concept <root>.\n",
      "2017-02-11 18:11:56,637 [INFO] Analyzing words for concept <rotten>.\n",
      "2017-02-11 18:11:56,656 [INFO] Analyzing words for concept <round>.\n",
      "2017-02-11 18:11:56,676 [INFO] Analyzing words for concept <rub>.\n",
      "SEQUENCE CLUSTERING:  64%|██████▍   | 128/200 [00:02<00:01, 55.98it/s]2017-02-11 18:11:56,697 [INFO] Analyzing words for concept <salt>.\n",
      "2017-02-11 18:11:56,715 [INFO] Analyzing words for concept <sand>.\n",
      "2017-02-11 18:11:56,731 [INFO] Analyzing words for concept <say>.\n",
      "2017-02-11 18:11:56,744 [INFO] Analyzing words for concept <scratch>.\n",
      "2017-02-11 18:11:56,764 [INFO] Analyzing words for concept <sea>.\n",
      "2017-02-11 18:11:56,779 [INFO] Analyzing words for concept <see>.\n",
      "2017-02-11 18:11:56,793 [INFO] Analyzing words for concept <seed>.\n",
      "SEQUENCE CLUSTERING:  68%|██████▊   | 135/200 [00:02<00:01, 56.89it/s]2017-02-11 18:11:56,816 [INFO] Analyzing words for concept <sew>.\n",
      "2017-02-11 18:11:56,832 [INFO] Analyzing words for concept <sharp>.\n",
      "2017-02-11 18:11:56,852 [INFO] Analyzing words for concept <short>.\n",
      "2017-02-11 18:11:56,870 [INFO] Analyzing words for concept <sing>.\n",
      "2017-02-11 18:11:56,892 [INFO] Analyzing words for concept <sit>.\n",
      "2017-02-11 18:11:56,911 [INFO] Analyzing words for concept <skin>.\n",
      "SEQUENCE CLUSTERING:  70%|███████   | 141/200 [00:02<00:01, 55.80it/s]2017-02-11 18:11:56,929 [INFO] Analyzing words for concept <sky>.\n",
      "2017-02-11 18:11:56,946 [INFO] Analyzing words for concept <sleep>.\n",
      "2017-02-11 18:11:56,961 [INFO] Analyzing words for concept <small>.\n",
      "2017-02-11 18:11:56,979 [INFO] Analyzing words for concept <smell>.\n",
      "2017-02-11 18:11:56,998 [INFO] Analyzing words for concept <smoke>.\n",
      "2017-02-11 18:11:57,015 [INFO] Analyzing words for concept <smooth>.\n",
      "SEQUENCE CLUSTERING:  74%|███████▎  | 147/200 [00:02<00:00, 55.68it/s]2017-02-11 18:11:57,037 [INFO] Analyzing words for concept <snake>.\n",
      "2017-02-11 18:11:57,057 [INFO] Analyzing words for concept <snow>.\n",
      "2017-02-11 18:11:57,071 [INFO] Analyzing words for concept <some>.\n",
      "2017-02-11 18:11:57,088 [INFO] Analyzing words for concept <spit>.\n",
      "2017-02-11 18:11:57,108 [INFO] Analyzing words for concept <split>.\n",
      "2017-02-11 18:11:57,127 [INFO] Analyzing words for concept <squeeze>.\n",
      "SEQUENCE CLUSTERING:  76%|███████▋  | 153/200 [00:02<00:00, 54.82it/s]2017-02-11 18:11:57,151 [INFO] Analyzing words for concept <stab>.\n",
      "2017-02-11 18:11:57,174 [INFO] Analyzing words for concept <stand>.\n",
      "2017-02-11 18:11:57,190 [INFO] Analyzing words for concept <star>.\n",
      "2017-02-11 18:11:57,208 [INFO] Analyzing words for concept <stick>.\n",
      "2017-02-11 18:11:57,226 [INFO] Analyzing words for concept <stone>.\n",
      "2017-02-11 18:11:57,241 [INFO] Analyzing words for concept <straight>.\n",
      "SEQUENCE CLUSTERING:  80%|███████▉  | 159/200 [00:02<00:00, 53.79it/s]2017-02-11 18:11:57,266 [INFO] Analyzing words for concept <suck>.\n",
      "2017-02-11 18:11:57,283 [INFO] Analyzing words for concept <sun>.\n",
      "2017-02-11 18:11:57,297 [INFO] Analyzing words for concept <swell>.\n",
      "2017-02-11 18:11:57,316 [INFO] Analyzing words for concept <swim>.\n",
      "2017-02-11 18:11:57,331 [INFO] Analyzing words for concept <tail>.\n",
      "2017-02-11 18:11:57,347 [INFO] Analyzing words for concept <that>.\n",
      "2017-02-11 18:11:57,361 [INFO] Analyzing words for concept <there>.\n",
      "SEQUENCE CLUSTERING:  83%|████████▎ | 166/200 [00:02<00:00, 56.11it/s]2017-02-11 18:11:57,378 [INFO] Analyzing words for concept <they>.\n",
      "2017-02-11 18:11:57,392 [INFO] Analyzing words for concept <thick>.\n",
      "2017-02-11 18:11:57,409 [INFO] Analyzing words for concept <thin>.\n",
      "2017-02-11 18:11:57,428 [INFO] Analyzing words for concept <think>.\n",
      "2017-02-11 18:11:57,446 [INFO] Analyzing words for concept <this>.\n",
      "2017-02-11 18:11:57,460 [INFO] Analyzing words for concept <thou>.\n",
      "2017-02-11 18:11:57,473 [INFO] Analyzing words for concept <three>.\n",
      "SEQUENCE CLUSTERING:  86%|████████▋ | 173/200 [00:03<00:00, 58.09it/s]2017-02-11 18:11:57,490 [INFO] Analyzing words for concept <throw>.\n",
      "2017-02-11 18:11:57,504 [INFO] Analyzing words for concept <tie>.\n",
      "2017-02-11 18:11:57,523 [INFO] Analyzing words for concept <tongue>.\n",
      "2017-02-11 18:11:57,539 [INFO] Analyzing words for concept <tooth>.\n",
      "2017-02-11 18:11:57,554 [INFO] Analyzing words for concept <tree>.\n",
      "2017-02-11 18:11:57,570 [INFO] Analyzing words for concept <true>.\n",
      "SEQUENCE CLUSTERING:  90%|████████▉ | 179/200 [00:03<00:00, 58.25it/s]2017-02-11 18:11:57,593 [INFO] Analyzing words for concept <two>.\n",
      "2017-02-11 18:11:57,607 [INFO] Analyzing words for concept <vomit>.\n",
      "2017-02-11 18:11:57,626 [INFO] Analyzing words for concept <wash>.\n",
      "2017-02-11 18:11:57,643 [INFO] Analyzing words for concept <water>.\n",
      "2017-02-11 18:11:57,657 [INFO] Analyzing words for concept <we>.\n",
      "2017-02-11 18:11:57,671 [INFO] Analyzing words for concept <wet>.\n",
      "2017-02-11 18:11:57,688 [INFO] Analyzing words for concept <what>.\n",
      "SEQUENCE CLUSTERING:  93%|█████████▎| 186/200 [00:03<00:00, 59.27it/s]2017-02-11 18:11:57,706 [INFO] Analyzing words for concept <white>.\n",
      "2017-02-11 18:11:57,724 [INFO] Analyzing words for concept <who>.\n",
      "2017-02-11 18:11:57,736 [INFO] Analyzing words for concept <wide>.\n",
      "2017-02-11 18:11:57,752 [INFO] Analyzing words for concept <wife>.\n",
      "2017-02-11 18:11:57,767 [INFO] Analyzing words for concept <wind>.\n",
      "2017-02-11 18:11:57,785 [INFO] Analyzing words for concept <wing>.\n",
      "SEQUENCE CLUSTERING:  96%|█████████▌| 192/200 [00:03<00:00, 59.33it/s]2017-02-11 18:11:57,806 [INFO] Analyzing words for concept <wipe>.\n",
      "2017-02-11 18:11:57,824 [INFO] Analyzing words for concept <with>.\n",
      "2017-02-11 18:11:57,838 [INFO] Analyzing words for concept <woman>.\n",
      "2017-02-11 18:11:57,857 [INFO] Analyzing words for concept <woods>.\n",
      "2017-02-11 18:11:57,875 [INFO] Analyzing words for concept <worm>.\n",
      "2017-02-11 18:11:57,893 [INFO] Analyzing words for concept <year>.\n",
      "SEQUENCE CLUSTERING:  99%|█████████▉| 198/200 [00:03<00:00, 58.50it/s]2017-02-11 18:11:57,913 [INFO] Analyzing words for concept <yellow>.\n",
      "2017-02-11 18:11:57,932 [INFO] Analyzing words for concept <you>.\n",
      "SEQUENCE CLUSTERING:   0%|          | 0/200 [00:00<?, ?it/s]          2017-02-11 18:11:57,957 [INFO] Analyzing words for concept <I>.\n",
      "2017-02-11 18:11:57,963 [INFO] Analyzing words for concept <all>.\n",
      "2017-02-11 18:11:57,969 [INFO] Analyzing words for concept <and>.\n",
      "2017-02-11 18:11:57,976 [INFO] Analyzing words for concept <animal>.\n",
      "2017-02-11 18:11:57,984 [INFO] Analyzing words for concept <ashes>.\n",
      "2017-02-11 18:11:57,990 [INFO] Analyzing words for concept <at>.\n",
      "2017-02-11 18:11:57,996 [INFO] Analyzing words for concept <back>.\n",
      "2017-02-11 18:11:58,003 [INFO] Analyzing words for concept <bad>.\n",
      "2017-02-11 18:11:58,010 [INFO] Analyzing words for concept <bark>.\n",
      "2017-02-11 18:11:58,017 [INFO] Analyzing words for concept <because>.\n",
      "2017-02-11 18:11:58,024 [INFO] Analyzing words for concept <belly>.\n",
      "2017-02-11 18:11:58,030 [INFO] Analyzing words for concept <big>.\n",
      "2017-02-11 18:11:58,037 [INFO] Analyzing words for concept <bird>.\n",
      "2017-02-11 18:11:58,044 [INFO] Analyzing words for concept <bite>.\n",
      "2017-02-11 18:11:58,050 [INFO] Analyzing words for concept <black>.\n",
      "SEQUENCE CLUSTERING:   8%|▊         | 15/200 [00:00<00:01, 149.56it/s]2017-02-11 18:11:58,061 [INFO] Analyzing words for concept <blood>.\n",
      "2017-02-11 18:11:58,067 [INFO] Analyzing words for concept <blow>.\n",
      "2017-02-11 18:11:58,074 [INFO] Analyzing words for concept <bone>.\n",
      "2017-02-11 18:11:58,080 [INFO] Analyzing words for concept <breast>.\n",
      "2017-02-11 18:11:58,087 [INFO] Analyzing words for concept <breathe>.\n",
      "2017-02-11 18:11:58,094 [INFO] Analyzing words for concept <burn>.\n",
      "2017-02-11 18:11:58,101 [INFO] Analyzing words for concept <child>.\n",
      "2017-02-11 18:11:58,107 [INFO] Analyzing words for concept <claw>.\n",
      "2017-02-11 18:11:58,114 [INFO] Analyzing words for concept <cloud>.\n",
      "2017-02-11 18:11:58,121 [INFO] Analyzing words for concept <cold>.\n",
      "2017-02-11 18:11:58,130 [INFO] Analyzing words for concept <come>.\n",
      "2017-02-11 18:11:58,137 [INFO] Analyzing words for concept <count>.\n",
      "2017-02-11 18:11:58,144 [INFO] Analyzing words for concept <cut>.\n",
      "2017-02-11 18:11:58,151 [INFO] Analyzing words for concept <day>.\n",
      "2017-02-11 18:11:58,157 [INFO] Analyzing words for concept <die>.\n",
      "SEQUENCE CLUSTERING:  15%|█▌        | 30/200 [00:00<00:01, 146.51it/s]2017-02-11 18:11:58,166 [INFO] Analyzing words for concept <dig>.\n",
      "2017-02-11 18:11:58,173 [INFO] Analyzing words for concept <dirty>.\n",
      "2017-02-11 18:11:58,180 [INFO] Analyzing words for concept <dog>.\n",
      "2017-02-11 18:11:58,186 [INFO] Analyzing words for concept <drink>.\n",
      "2017-02-11 18:11:58,193 [INFO] Analyzing words for concept <dry>.\n",
      "2017-02-11 18:11:58,199 [INFO] Analyzing words for concept <dull>.\n",
      "2017-02-11 18:11:58,206 [INFO] Analyzing words for concept <dust>.\n",
      "2017-02-11 18:11:58,212 [INFO] Analyzing words for concept <ear>.\n",
      "2017-02-11 18:11:58,218 [INFO] Analyzing words for concept <earth>.\n",
      "2017-02-11 18:11:58,225 [INFO] Analyzing words for concept <eat>.\n",
      "2017-02-11 18:11:58,230 [INFO] Analyzing words for concept <egg>.\n",
      "2017-02-11 18:11:58,238 [INFO] Analyzing words for concept <eye>.\n",
      "2017-02-11 18:11:58,243 [INFO] Analyzing words for concept <fall>.\n",
      "2017-02-11 18:11:58,249 [INFO] Analyzing words for concept <far>.\n",
      "2017-02-11 18:11:58,256 [INFO] Analyzing words for concept <father>.\n",
      "2017-02-11 18:11:58,263 [INFO] Analyzing words for concept <feather>.\n",
      "SEQUENCE CLUSTERING:  23%|██▎       | 46/200 [00:00<00:01, 147.94it/s]2017-02-11 18:11:58,274 [INFO] Analyzing words for concept <few>.\n",
      "2017-02-11 18:11:58,281 [INFO] Analyzing words for concept <fight>.\n",
      "2017-02-11 18:11:58,288 [INFO] Analyzing words for concept <fire>.\n",
      "2017-02-11 18:11:58,294 [INFO] Analyzing words for concept <fish>.\n",
      "2017-02-11 18:11:58,301 [INFO] Analyzing words for concept <five>.\n",
      "2017-02-11 18:11:58,309 [INFO] Analyzing words for concept <flow>.\n",
      "2017-02-11 18:11:58,315 [INFO] Analyzing words for concept <flower>.\n",
      "2017-02-11 18:11:58,322 [INFO] Analyzing words for concept <fly>.\n",
      "2017-02-11 18:11:58,329 [INFO] Analyzing words for concept <fog>.\n",
      "2017-02-11 18:11:58,335 [INFO] Analyzing words for concept <foot>.\n",
      "2017-02-11 18:11:58,342 [INFO] Analyzing words for concept <four>.\n",
      "2017-02-11 18:11:58,349 [INFO] Analyzing words for concept <freeze>.\n",
      "2017-02-11 18:11:58,356 [INFO] Analyzing words for concept <fruit>.\n",
      "2017-02-11 18:11:58,363 [INFO] Analyzing words for concept <full>.\n",
      "2017-02-11 18:11:58,370 [INFO] Analyzing words for concept <give>.\n",
      "SEQUENCE CLUSTERING:  30%|███       | 61/200 [00:00<00:00, 145.73it/s]2017-02-11 18:11:58,380 [INFO] Analyzing words for concept <go>.\n",
      "2017-02-11 18:11:58,387 [INFO] Analyzing words for concept <good>.\n",
      "2017-02-11 18:11:58,393 [INFO] Analyzing words for concept <grass>.\n",
      "2017-02-11 18:11:58,400 [INFO] Analyzing words for concept <grease>.\n",
      "2017-02-11 18:11:58,407 [INFO] Analyzing words for concept <green>.\n",
      "2017-02-11 18:11:58,414 [INFO] Analyzing words for concept <guts>.\n",
      "2017-02-11 18:11:58,421 [INFO] Analyzing words for concept <hair>.\n",
      "2017-02-11 18:11:58,428 [INFO] Analyzing words for concept <hand>.\n",
      "2017-02-11 18:11:58,434 [INFO] Analyzing words for concept <he>.\n",
      "2017-02-11 18:11:58,441 [INFO] Analyzing words for concept <head>.\n",
      "2017-02-11 18:11:58,447 [INFO] Analyzing words for concept <hear>.\n",
      "2017-02-11 18:11:58,454 [INFO] Analyzing words for concept <heart>.\n",
      "2017-02-11 18:11:58,461 [INFO] Analyzing words for concept <heavy>.\n",
      "2017-02-11 18:11:58,468 [INFO] Analyzing words for concept <here>.\n",
      "2017-02-11 18:11:58,475 [INFO] Analyzing words for concept <hit>.\n",
      "SEQUENCE CLUSTERING:  38%|███▊      | 76/200 [00:00<00:00, 144.94it/s]2017-02-11 18:11:58,484 [INFO] Analyzing words for concept <hold>.\n",
      "2017-02-11 18:11:58,490 [INFO] Analyzing words for concept <horn>.\n",
      "2017-02-11 18:11:58,497 [INFO] Analyzing words for concept <hot>.\n",
      "2017-02-11 18:11:58,503 [INFO] Analyzing words for concept <human>.\n",
      "2017-02-11 18:11:58,511 [INFO] Analyzing words for concept <hunt>.\n",
      "2017-02-11 18:11:58,517 [INFO] Analyzing words for concept <husband>.\n",
      "2017-02-11 18:11:58,524 [INFO] Analyzing words for concept <ice>.\n",
      "2017-02-11 18:11:58,530 [INFO] Analyzing words for concept <if>.\n",
      "2017-02-11 18:11:58,537 [INFO] Analyzing words for concept <in>.\n",
      "2017-02-11 18:11:58,543 [INFO] Analyzing words for concept <kill>.\n",
      "2017-02-11 18:11:58,550 [INFO] Analyzing words for concept <knee>.\n",
      "2017-02-11 18:11:58,563 [INFO] Analyzing words for concept <knife>.\n",
      "2017-02-11 18:11:58,569 [INFO] Analyzing words for concept <know>.\n",
      "2017-02-11 18:11:58,575 [INFO] Analyzing words for concept <lake>.\n",
      "SEQUENCE CLUSTERING:  45%|████▌     | 90/200 [00:00<00:00, 143.26it/s]2017-02-11 18:11:58,586 [INFO] Analyzing words for concept <laugh>.\n",
      "2017-02-11 18:11:58,591 [INFO] Analyzing words for concept <leaf>.\n",
      "2017-02-11 18:11:58,599 [INFO] Analyzing words for concept <left>.\n",
      "2017-02-11 18:11:58,605 [INFO] Analyzing words for concept <lie>.\n",
      "2017-02-11 18:11:58,611 [INFO] Analyzing words for concept <liver>.\n",
      "2017-02-11 18:11:58,617 [INFO] Analyzing words for concept <long>.\n",
      "2017-02-11 18:11:58,624 [INFO] Analyzing words for concept <louse>.\n",
      "2017-02-11 18:11:58,630 [INFO] Analyzing words for concept <man>.\n",
      "2017-02-11 18:11:58,636 [INFO] Analyzing words for concept <many>.\n",
      "2017-02-11 18:11:58,643 [INFO] Analyzing words for concept <meat>.\n",
      "2017-02-11 18:11:58,649 [INFO] Analyzing words for concept <moon>.\n",
      "2017-02-11 18:11:58,656 [INFO] Analyzing words for concept <mother>.\n",
      "2017-02-11 18:11:58,662 [INFO] Analyzing words for concept <mountain>.\n",
      "2017-02-11 18:11:58,670 [INFO] Analyzing words for concept <mouth>.\n",
      "2017-02-11 18:11:58,677 [INFO] Analyzing words for concept <name>.\n",
      "SEQUENCE CLUSTERING:  52%|█████▎    | 105/200 [00:00<00:00, 145.01it/s]2017-02-11 18:11:58,685 [INFO] Analyzing words for concept <narrow>.\n",
      "2017-02-11 18:11:58,694 [INFO] Analyzing words for concept <near>.\n",
      "2017-02-11 18:11:58,700 [INFO] Analyzing words for concept <neck>.\n",
      "2017-02-11 18:11:58,706 [INFO] Analyzing words for concept <new>.\n",
      "2017-02-11 18:11:58,712 [INFO] Analyzing words for concept <night>.\n",
      "2017-02-11 18:11:58,717 [INFO] Analyzing words for concept <nose>.\n",
      "2017-02-11 18:11:58,725 [INFO] Analyzing words for concept <not>.\n",
      "2017-02-11 18:11:58,732 [INFO] Analyzing words for concept <now>.\n",
      "2017-02-11 18:11:58,738 [INFO] Analyzing words for concept <old>.\n",
      "2017-02-11 18:11:58,745 [INFO] Analyzing words for concept <one>.\n",
      "2017-02-11 18:11:58,751 [INFO] Analyzing words for concept <other>.\n",
      "2017-02-11 18:11:58,758 [INFO] Analyzing words for concept <path>.\n",
      "2017-02-11 18:11:58,764 [INFO] Analyzing words for concept <play>.\n",
      "2017-02-11 18:11:58,771 [INFO] Analyzing words for concept <pull>.\n",
      "2017-02-11 18:11:58,778 [INFO] Analyzing words for concept <push>.\n",
      "SEQUENCE CLUSTERING:  60%|██████    | 120/200 [00:00<00:00, 145.55it/s]2017-02-11 18:11:58,788 [INFO] Analyzing words for concept <rain>.\n",
      "2017-02-11 18:11:58,795 [INFO] Analyzing words for concept <red>.\n",
      "2017-02-11 18:11:58,801 [INFO] Analyzing words for concept <right>.\n",
      "2017-02-11 18:11:58,808 [INFO] Analyzing words for concept <river>.\n",
      "2017-02-11 18:11:58,815 [INFO] Analyzing words for concept <root>.\n",
      "2017-02-11 18:11:58,821 [INFO] Analyzing words for concept <rotten>.\n",
      "2017-02-11 18:11:58,828 [INFO] Analyzing words for concept <round>.\n",
      "2017-02-11 18:11:58,836 [INFO] Analyzing words for concept <rub>.\n",
      "2017-02-11 18:11:58,843 [INFO] Analyzing words for concept <salt>.\n",
      "2017-02-11 18:11:58,849 [INFO] Analyzing words for concept <sand>.\n",
      "2017-02-11 18:11:58,858 [INFO] Analyzing words for concept <say>.\n",
      "2017-02-11 18:11:58,863 [INFO] Analyzing words for concept <scratch>.\n",
      "2017-02-11 18:11:58,870 [INFO] Analyzing words for concept <sea>.\n",
      "2017-02-11 18:11:58,877 [INFO] Analyzing words for concept <see>.\n",
      "2017-02-11 18:11:58,883 [INFO] Analyzing words for concept <seed>.\n",
      "SEQUENCE CLUSTERING:  68%|██████▊   | 135/200 [00:00<00:00, 143.67it/s]2017-02-11 18:11:58,894 [INFO] Analyzing words for concept <sew>.\n",
      "2017-02-11 18:11:58,900 [INFO] Analyzing words for concept <sharp>.\n",
      "2017-02-11 18:11:58,907 [INFO] Analyzing words for concept <short>.\n",
      "2017-02-11 18:11:58,914 [INFO] Analyzing words for concept <sing>.\n",
      "2017-02-11 18:11:58,921 [INFO] Analyzing words for concept <sit>.\n",
      "2017-02-11 18:11:58,928 [INFO] Analyzing words for concept <skin>.\n",
      "2017-02-11 18:11:58,934 [INFO] Analyzing words for concept <sky>.\n",
      "2017-02-11 18:11:58,940 [INFO] Analyzing words for concept <sleep>.\n",
      "2017-02-11 18:11:58,947 [INFO] Analyzing words for concept <small>.\n",
      "2017-02-11 18:11:58,954 [INFO] Analyzing words for concept <smell>.\n",
      "2017-02-11 18:11:58,960 [INFO] Analyzing words for concept <smoke>.\n",
      "2017-02-11 18:11:58,967 [INFO] Analyzing words for concept <smooth>.\n",
      "2017-02-11 18:11:58,974 [INFO] Analyzing words for concept <snake>.\n",
      "2017-02-11 18:11:58,981 [INFO] Analyzing words for concept <snow>.\n",
      "2017-02-11 18:11:58,988 [INFO] Analyzing words for concept <some>.\n",
      "SEQUENCE CLUSTERING:  75%|███████▌  | 150/200 [00:01<00:00, 144.84it/s]2017-02-11 18:11:58,998 [INFO] Analyzing words for concept <spit>.\n",
      "2017-02-11 18:11:59,003 [INFO] Analyzing words for concept <split>.\n",
      "2017-02-11 18:11:59,010 [INFO] Analyzing words for concept <squeeze>.\n",
      "2017-02-11 18:11:59,018 [INFO] Analyzing words for concept <stab>.\n",
      "2017-02-11 18:11:59,026 [INFO] Analyzing words for concept <stand>.\n",
      "2017-02-11 18:11:59,033 [INFO] Analyzing words for concept <star>.\n",
      "2017-02-11 18:11:59,040 [INFO] Analyzing words for concept <stick>.\n",
      "2017-02-11 18:11:59,046 [INFO] Analyzing words for concept <stone>.\n",
      "2017-02-11 18:11:59,053 [INFO] Analyzing words for concept <straight>.\n",
      "2017-02-11 18:11:59,061 [INFO] Analyzing words for concept <suck>.\n",
      "2017-02-11 18:11:59,067 [INFO] Analyzing words for concept <sun>.\n",
      "2017-02-11 18:11:59,074 [INFO] Analyzing words for concept <swell>.\n",
      "2017-02-11 18:11:59,080 [INFO] Analyzing words for concept <swim>.\n",
      "2017-02-11 18:11:59,087 [INFO] Analyzing words for concept <tail>.\n",
      "2017-02-11 18:11:59,094 [INFO] Analyzing words for concept <that>.\n",
      "SEQUENCE CLUSTERING:  82%|████████▎ | 165/200 [00:01<00:00, 143.04it/s]2017-02-11 18:11:59,103 [INFO] Analyzing words for concept <there>.\n",
      "2017-02-11 18:11:59,110 [INFO] Analyzing words for concept <they>.\n",
      "2017-02-11 18:11:59,116 [INFO] Analyzing words for concept <thick>.\n",
      "2017-02-11 18:11:59,123 [INFO] Analyzing words for concept <thin>.\n",
      "2017-02-11 18:11:59,130 [INFO] Analyzing words for concept <think>.\n",
      "2017-02-11 18:11:59,137 [INFO] Analyzing words for concept <this>.\n",
      "2017-02-11 18:11:59,143 [INFO] Analyzing words for concept <thou>.\n",
      "2017-02-11 18:11:59,150 [INFO] Analyzing words for concept <three>.\n",
      "2017-02-11 18:11:59,156 [INFO] Analyzing words for concept <throw>.\n",
      "2017-02-11 18:11:59,163 [INFO] Analyzing words for concept <tie>.\n",
      "2017-02-11 18:11:59,169 [INFO] Analyzing words for concept <tongue>.\n",
      "2017-02-11 18:11:59,176 [INFO] Analyzing words for concept <tooth>.\n",
      "2017-02-11 18:11:59,182 [INFO] Analyzing words for concept <tree>.\n",
      "2017-02-11 18:11:59,189 [INFO] Analyzing words for concept <true>.\n",
      "2017-02-11 18:11:59,196 [INFO] Analyzing words for concept <two>.\n",
      "SEQUENCE CLUSTERING:  90%|█████████ | 180/200 [00:01<00:00, 144.92it/s]2017-02-11 18:11:59,206 [INFO] Analyzing words for concept <vomit>.\n",
      "2017-02-11 18:11:59,212 [INFO] Analyzing words for concept <wash>.\n",
      "2017-02-11 18:11:59,219 [INFO] Analyzing words for concept <water>.\n",
      "2017-02-11 18:11:59,227 [INFO] Analyzing words for concept <we>.\n",
      "2017-02-11 18:11:59,232 [INFO] Analyzing words for concept <wet>.\n",
      "2017-02-11 18:11:59,241 [INFO] Analyzing words for concept <what>.\n",
      "2017-02-11 18:11:59,248 [INFO] Analyzing words for concept <white>.\n",
      "2017-02-11 18:11:59,253 [INFO] Analyzing words for concept <who>.\n",
      "2017-02-11 18:11:59,261 [INFO] Analyzing words for concept <wide>.\n",
      "2017-02-11 18:11:59,266 [INFO] Analyzing words for concept <wife>.\n",
      "2017-02-11 18:11:59,273 [INFO] Analyzing words for concept <wind>.\n",
      "2017-02-11 18:11:59,280 [INFO] Analyzing words for concept <wing>.\n",
      "2017-02-11 18:11:59,289 [INFO] Analyzing words for concept <wipe>.\n",
      "2017-02-11 18:11:59,295 [INFO] Analyzing words for concept <with>.\n",
      "2017-02-11 18:11:59,304 [INFO] Analyzing words for concept <woman>.\n",
      "SEQUENCE CLUSTERING:  98%|█████████▊| 195/200 [00:01<00:00, 143.32it/s]2017-02-11 18:11:59,313 [INFO] Analyzing words for concept <woods>.\n",
      "2017-02-11 18:11:59,320 [INFO] Analyzing words for concept <worm>.\n",
      "2017-02-11 18:11:59,328 [INFO] Analyzing words for concept <year>.\n",
      "2017-02-11 18:11:59,335 [INFO] Analyzing words for concept <yellow>.\n",
      "2017-02-11 18:11:59,342 [INFO] Analyzing words for concept <you>.\n",
      "SEQUENCE CLUSTERING:   0%|          | 0/200 [00:00<?, ?it/s]           2017-02-11 18:11:59,366 [INFO] Analyzing words for concept <I>.\n",
      "2017-02-11 18:11:59,373 [INFO] Analyzing words for concept <all>.\n",
      "2017-02-11 18:11:59,381 [INFO] Analyzing words for concept <and>.\n",
      "2017-02-11 18:11:59,390 [INFO] Analyzing words for concept <animal>.\n",
      "2017-02-11 18:11:59,399 [INFO] Analyzing words for concept <ashes>.\n",
      "2017-02-11 18:11:59,404 [INFO] Analyzing words for concept <at>.\n",
      "2017-02-11 18:11:59,411 [INFO] Analyzing words for concept <back>.\n",
      "2017-02-11 18:11:59,418 [INFO] Analyzing words for concept <bad>.\n",
      "2017-02-11 18:11:59,425 [INFO] Analyzing words for concept <bark>.\n",
      "2017-02-11 18:11:59,438 [INFO] Analyzing words for concept <because>.\n",
      "2017-02-11 18:11:59,445 [INFO] Analyzing words for concept <belly>.\n",
      "2017-02-11 18:11:59,453 [INFO] Analyzing words for concept <big>.\n",
      "2017-02-11 18:11:59,460 [INFO] Analyzing words for concept <bird>.\n",
      "SEQUENCE CLUSTERING:   6%|▋         | 13/200 [00:00<00:01, 123.51it/s]2017-02-11 18:11:59,474 [INFO] Analyzing words for concept <bite>.\n",
      "2017-02-11 18:11:59,480 [INFO] Analyzing words for concept <black>.\n",
      "2017-02-11 18:11:59,486 [INFO] Analyzing words for concept <blood>.\n",
      "2017-02-11 18:11:59,493 [INFO] Analyzing words for concept <blow>.\n",
      "2017-02-11 18:11:59,498 [INFO] Analyzing words for concept <bone>.\n",
      "2017-02-11 18:11:59,504 [INFO] Analyzing words for concept <breast>.\n",
      "2017-02-11 18:11:59,510 [INFO] Analyzing words for concept <breathe>.\n",
      "2017-02-11 18:11:59,517 [INFO] Analyzing words for concept <burn>.\n",
      "2017-02-11 18:11:59,523 [INFO] Analyzing words for concept <child>.\n",
      "2017-02-11 18:11:59,529 [INFO] Analyzing words for concept <claw>.\n",
      "2017-02-11 18:11:59,535 [INFO] Analyzing words for concept <cloud>.\n",
      "2017-02-11 18:11:59,540 [INFO] Analyzing words for concept <cold>.\n",
      "2017-02-11 18:11:59,546 [INFO] Analyzing words for concept <come>.\n",
      "2017-02-11 18:11:59,552 [INFO] Analyzing words for concept <count>.\n",
      "2017-02-11 18:11:59,568 [INFO] Analyzing words for concept <cut>.\n",
      "SEQUENCE CLUSTERING:  14%|█▍        | 28/200 [00:00<00:01, 129.41it/s]2017-02-11 18:11:59,578 [INFO] Analyzing words for concept <day>.\n",
      "2017-02-11 18:11:59,584 [INFO] Analyzing words for concept <die>.\n",
      "2017-02-11 18:11:59,590 [INFO] Analyzing words for concept <dig>.\n",
      "2017-02-11 18:11:59,598 [INFO] Analyzing words for concept <dirty>.\n",
      "2017-02-11 18:11:59,603 [INFO] Analyzing words for concept <dog>.\n",
      "2017-02-11 18:11:59,611 [INFO] Analyzing words for concept <drink>.\n",
      "2017-02-11 18:11:59,616 [INFO] Analyzing words for concept <dry>.\n",
      "2017-02-11 18:11:59,622 [INFO] Analyzing words for concept <dull>.\n",
      "2017-02-11 18:11:59,629 [INFO] Analyzing words for concept <dust>.\n",
      "2017-02-11 18:11:59,637 [INFO] Analyzing words for concept <ear>.\n",
      "2017-02-11 18:11:59,642 [INFO] Analyzing words for concept <earth>.\n",
      "2017-02-11 18:11:59,649 [INFO] Analyzing words for concept <eat>.\n",
      "2017-02-11 18:11:59,655 [INFO] Analyzing words for concept <egg>.\n",
      "2017-02-11 18:11:59,661 [INFO] Analyzing words for concept <eye>.\n",
      "2017-02-11 18:11:59,668 [INFO] Analyzing words for concept <fall>.\n",
      "SEQUENCE CLUSTERING:  22%|██▏       | 43/200 [00:00<00:01, 134.84it/s]2017-02-11 18:11:59,678 [INFO] Analyzing words for concept <far>.\n",
      "2017-02-11 18:11:59,684 [INFO] Analyzing words for concept <father>.\n",
      "2017-02-11 18:11:59,691 [INFO] Analyzing words for concept <feather>.\n",
      "2017-02-11 18:11:59,697 [INFO] Analyzing words for concept <few>.\n",
      "2017-02-11 18:11:59,703 [INFO] Analyzing words for concept <fight>.\n",
      "2017-02-11 18:11:59,710 [INFO] Analyzing words for concept <fire>.\n",
      "2017-02-11 18:11:59,716 [INFO] Analyzing words for concept <fish>.\n",
      "2017-02-11 18:11:59,723 [INFO] Analyzing words for concept <five>.\n",
      "2017-02-11 18:11:59,729 [INFO] Analyzing words for concept <flow>.\n",
      "2017-02-11 18:11:59,736 [INFO] Analyzing words for concept <flower>.\n",
      "2017-02-11 18:11:59,744 [INFO] Analyzing words for concept <fly>.\n",
      "2017-02-11 18:11:59,751 [INFO] Analyzing words for concept <fog>.\n",
      "2017-02-11 18:11:59,758 [INFO] Analyzing words for concept <foot>.\n",
      "2017-02-11 18:11:59,764 [INFO] Analyzing words for concept <four>.\n",
      "2017-02-11 18:11:59,771 [INFO] Analyzing words for concept <freeze>.\n",
      "SEQUENCE CLUSTERING:  29%|██▉       | 58/200 [00:00<00:01, 137.64it/s]2017-02-11 18:11:59,784 [INFO] Analyzing words for concept <fruit>.\n",
      "2017-02-11 18:11:59,789 [INFO] Analyzing words for concept <full>.\n",
      "2017-02-11 18:11:59,800 [INFO] Analyzing words for concept <give>.\n",
      "2017-02-11 18:11:59,810 [INFO] Analyzing words for concept <go>.\n",
      "2017-02-11 18:11:59,818 [INFO] Analyzing words for concept <good>.\n",
      "2017-02-11 18:11:59,823 [INFO] Analyzing words for concept <grass>.\n",
      "2017-02-11 18:11:59,831 [INFO] Analyzing words for concept <grease>.\n",
      "2017-02-11 18:11:59,838 [INFO] Analyzing words for concept <green>.\n",
      "2017-02-11 18:11:59,846 [INFO] Analyzing words for concept <guts>.\n",
      "2017-02-11 18:11:59,857 [INFO] Analyzing words for concept <hair>.\n",
      "2017-02-11 18:11:59,870 [INFO] Analyzing words for concept <hand>.\n",
      "2017-02-11 18:11:59,875 [INFO] Analyzing words for concept <he>.\n",
      "SEQUENCE CLUSTERING:  35%|███▌      | 70/200 [00:00<00:01, 129.07it/s]2017-02-11 18:11:59,887 [INFO] Analyzing words for concept <head>.\n",
      "2017-02-11 18:11:59,894 [INFO] Analyzing words for concept <hear>.\n",
      "2017-02-11 18:11:59,902 [INFO] Analyzing words for concept <heart>.\n",
      "2017-02-11 18:11:59,909 [INFO] Analyzing words for concept <heavy>.\n",
      "2017-02-11 18:11:59,916 [INFO] Analyzing words for concept <here>.\n",
      "2017-02-11 18:11:59,925 [INFO] Analyzing words for concept <hit>.\n",
      "2017-02-11 18:11:59,932 [INFO] Analyzing words for concept <hold>.\n",
      "2017-02-11 18:11:59,939 [INFO] Analyzing words for concept <horn>.\n",
      "2017-02-11 18:11:59,946 [INFO] Analyzing words for concept <hot>.\n",
      "2017-02-11 18:11:59,954 [INFO] Analyzing words for concept <human>.\n",
      "2017-02-11 18:11:59,961 [INFO] Analyzing words for concept <hunt>.\n",
      "2017-02-11 18:11:59,968 [INFO] Analyzing words for concept <husband>.\n",
      "2017-02-11 18:11:59,977 [INFO] Analyzing words for concept <ice>.\n",
      "2017-02-11 18:11:59,984 [INFO] Analyzing words for concept <if>.\n",
      "SEQUENCE CLUSTERING:  42%|████▏     | 84/200 [00:00<00:00, 129.32it/s]2017-02-11 18:11:59,996 [INFO] Analyzing words for concept <in>.\n",
      "2017-02-11 18:12:00,003 [INFO] Analyzing words for concept <kill>.\n",
      "2017-02-11 18:12:00,011 [INFO] Analyzing words for concept <knee>.\n",
      "2017-02-11 18:12:00,018 [INFO] Analyzing words for concept <knife>.\n",
      "2017-02-11 18:12:00,026 [INFO] Analyzing words for concept <know>.\n",
      "2017-02-11 18:12:00,033 [INFO] Analyzing words for concept <lake>.\n",
      "2017-02-11 18:12:00,041 [INFO] Analyzing words for concept <laugh>.\n",
      "2017-02-11 18:12:00,049 [INFO] Analyzing words for concept <leaf>.\n",
      "2017-02-11 18:12:00,058 [INFO] Analyzing words for concept <left>.\n",
      "2017-02-11 18:12:00,067 [INFO] Analyzing words for concept <lie>.\n",
      "2017-02-11 18:12:00,074 [INFO] Analyzing words for concept <liver>.\n",
      "2017-02-11 18:12:00,081 [INFO] Analyzing words for concept <long>.\n",
      "2017-02-11 18:12:00,089 [INFO] Analyzing words for concept <louse>.\n",
      "SEQUENCE CLUSTERING:  48%|████▊     | 97/200 [00:00<00:00, 128.10it/s]2017-02-11 18:12:00,101 [INFO] Analyzing words for concept <man>.\n",
      "2017-02-11 18:12:00,106 [INFO] Analyzing words for concept <many>.\n",
      "2017-02-11 18:12:00,113 [INFO] Analyzing words for concept <meat>.\n",
      "2017-02-11 18:12:00,121 [INFO] Analyzing words for concept <moon>.\n",
      "2017-02-11 18:12:00,128 [INFO] Analyzing words for concept <mother>.\n",
      "2017-02-11 18:12:00,136 [INFO] Analyzing words for concept <mountain>.\n",
      "2017-02-11 18:12:00,143 [INFO] Analyzing words for concept <mouth>.\n",
      "2017-02-11 18:12:00,151 [INFO] Analyzing words for concept <name>.\n",
      "2017-02-11 18:12:00,158 [INFO] Analyzing words for concept <narrow>.\n",
      "2017-02-11 18:12:00,166 [INFO] Analyzing words for concept <near>.\n",
      "2017-02-11 18:12:00,173 [INFO] Analyzing words for concept <neck>.\n",
      "2017-02-11 18:12:00,181 [INFO] Analyzing words for concept <new>.\n",
      "2017-02-11 18:12:00,188 [INFO] Analyzing words for concept <night>.\n",
      "2017-02-11 18:12:00,196 [INFO] Analyzing words for concept <nose>.\n",
      "SEQUENCE CLUSTERING:  56%|█████▌    | 111/200 [00:00<00:00, 128.31it/s]2017-02-11 18:12:00,207 [INFO] Analyzing words for concept <not>.\n",
      "2017-02-11 18:12:00,214 [INFO] Analyzing words for concept <now>.\n",
      "2017-02-11 18:12:00,221 [INFO] Analyzing words for concept <old>.\n",
      "2017-02-11 18:12:00,228 [INFO] Analyzing words for concept <one>.\n",
      "2017-02-11 18:12:00,235 [INFO] Analyzing words for concept <other>.\n",
      "2017-02-11 18:12:00,243 [INFO] Analyzing words for concept <path>.\n",
      "2017-02-11 18:12:00,253 [INFO] Analyzing words for concept <play>.\n",
      "2017-02-11 18:12:00,258 [INFO] Analyzing words for concept <pull>.\n",
      "2017-02-11 18:12:00,264 [INFO] Analyzing words for concept <push>.\n",
      "2017-02-11 18:12:00,272 [INFO] Analyzing words for concept <rain>.\n",
      "2017-02-11 18:12:00,279 [INFO] Analyzing words for concept <red>.\n",
      "2017-02-11 18:12:00,286 [INFO] Analyzing words for concept <right>.\n",
      "2017-02-11 18:12:00,294 [INFO] Analyzing words for concept <river>.\n",
      "2017-02-11 18:12:00,301 [INFO] Analyzing words for concept <root>.\n",
      "SEQUENCE CLUSTERING:  62%|██████▎   | 125/200 [00:00<00:00, 130.31it/s]2017-02-11 18:12:00,312 [INFO] Analyzing words for concept <rotten>.\n",
      "2017-02-11 18:12:00,320 [INFO] Analyzing words for concept <round>.\n",
      "2017-02-11 18:12:00,328 [INFO] Analyzing words for concept <rub>.\n",
      "2017-02-11 18:12:00,335 [INFO] Analyzing words for concept <salt>.\n",
      "2017-02-11 18:12:00,346 [INFO] Analyzing words for concept <sand>.\n",
      "2017-02-11 18:12:00,352 [INFO] Analyzing words for concept <say>.\n",
      "2017-02-11 18:12:00,360 [INFO] Analyzing words for concept <scratch>.\n",
      "2017-02-11 18:12:00,370 [INFO] Analyzing words for concept <sea>.\n",
      "2017-02-11 18:12:00,377 [INFO] Analyzing words for concept <see>.\n",
      "2017-02-11 18:12:00,382 [INFO] Analyzing words for concept <seed>.\n",
      "2017-02-11 18:12:00,390 [INFO] Analyzing words for concept <sew>.\n",
      "2017-02-11 18:12:00,397 [INFO] Analyzing words for concept <sharp>.\n",
      "2017-02-11 18:12:00,404 [INFO] Analyzing words for concept <short>.\n",
      "SEQUENCE CLUSTERING:  69%|██████▉   | 138/200 [00:01<00:00, 129.25it/s]2017-02-11 18:12:00,415 [INFO] Analyzing words for concept <sing>.\n",
      "2017-02-11 18:12:00,422 [INFO] Analyzing words for concept <sit>.\n",
      "2017-02-11 18:12:00,429 [INFO] Analyzing words for concept <skin>.\n",
      "2017-02-11 18:12:00,439 [INFO] Analyzing words for concept <sky>.\n",
      "2017-02-11 18:12:00,445 [INFO] Analyzing words for concept <sleep>.\n",
      "2017-02-11 18:12:00,453 [INFO] Analyzing words for concept <small>.\n",
      "2017-02-11 18:12:00,460 [INFO] Analyzing words for concept <smell>.\n",
      "2017-02-11 18:12:00,467 [INFO] Analyzing words for concept <smoke>.\n",
      "2017-02-11 18:12:00,474 [INFO] Analyzing words for concept <smooth>.\n",
      "2017-02-11 18:12:00,480 [INFO] Analyzing words for concept <snake>.\n",
      "2017-02-11 18:12:00,490 [INFO] Analyzing words for concept <snow>.\n",
      "2017-02-11 18:12:00,497 [INFO] Analyzing words for concept <some>.\n",
      "2017-02-11 18:12:00,504 [INFO] Analyzing words for concept <spit>.\n",
      "SEQUENCE CLUSTERING:  76%|███████▌  | 151/200 [00:01<00:00, 129.15it/s]2017-02-11 18:12:00,516 [INFO] Analyzing words for concept <split>.\n",
      "2017-02-11 18:12:00,523 [INFO] Analyzing words for concept <squeeze>.\n",
      "2017-02-11 18:12:00,530 [INFO] Analyzing words for concept <stab>.\n",
      "2017-02-11 18:12:00,538 [INFO] Analyzing words for concept <stand>.\n",
      "2017-02-11 18:12:00,545 [INFO] Analyzing words for concept <star>.\n",
      "2017-02-11 18:12:00,554 [INFO] Analyzing words for concept <stick>.\n",
      "2017-02-11 18:12:00,564 [INFO] Analyzing words for concept <stone>.\n",
      "2017-02-11 18:12:00,572 [INFO] Analyzing words for concept <straight>.\n",
      "2017-02-11 18:12:00,579 [INFO] Analyzing words for concept <suck>.\n",
      "2017-02-11 18:12:00,587 [INFO] Analyzing words for concept <sun>.\n",
      "2017-02-11 18:12:00,594 [INFO] Analyzing words for concept <swell>.\n",
      "2017-02-11 18:12:00,601 [INFO] Analyzing words for concept <swim>.\n",
      "2017-02-11 18:12:00,609 [INFO] Analyzing words for concept <tail>.\n",
      "SEQUENCE CLUSTERING:  82%|████████▏ | 164/200 [00:01<00:00, 126.62it/s]2017-02-11 18:12:00,621 [INFO] Analyzing words for concept <that>.\n",
      "2017-02-11 18:12:00,628 [INFO] Analyzing words for concept <there>.\n",
      "2017-02-11 18:12:00,635 [INFO] Analyzing words for concept <they>.\n",
      "2017-02-11 18:12:00,642 [INFO] Analyzing words for concept <thick>.\n",
      "2017-02-11 18:12:00,650 [INFO] Analyzing words for concept <thin>.\n",
      "2017-02-11 18:12:00,660 [INFO] Analyzing words for concept <think>.\n",
      "2017-02-11 18:12:00,666 [INFO] Analyzing words for concept <this>.\n",
      "2017-02-11 18:12:00,677 [INFO] Analyzing words for concept <thou>.\n",
      "2017-02-11 18:12:00,682 [INFO] Analyzing words for concept <three>.\n",
      "2017-02-11 18:12:00,691 [INFO] Analyzing words for concept <throw>.\n",
      "2017-02-11 18:12:00,699 [INFO] Analyzing words for concept <tie>.\n",
      "2017-02-11 18:12:00,708 [INFO] Analyzing words for concept <tongue>.\n",
      "2017-02-11 18:12:00,715 [INFO] Analyzing words for concept <tooth>.\n",
      "SEQUENCE CLUSTERING:  88%|████████▊ | 177/200 [00:01<00:00, 125.31it/s]2017-02-11 18:12:00,729 [INFO] Analyzing words for concept <tree>.\n",
      "2017-02-11 18:12:00,734 [INFO] Analyzing words for concept <true>.\n",
      "2017-02-11 18:12:00,741 [INFO] Analyzing words for concept <two>.\n",
      "2017-02-11 18:12:00,748 [INFO] Analyzing words for concept <vomit>.\n",
      "2017-02-11 18:12:00,757 [INFO] Analyzing words for concept <wash>.\n",
      "2017-02-11 18:12:00,768 [INFO] Analyzing words for concept <water>.\n",
      "2017-02-11 18:12:00,773 [INFO] Analyzing words for concept <we>.\n",
      "2017-02-11 18:12:00,782 [INFO] Analyzing words for concept <wet>.\n",
      "2017-02-11 18:12:00,789 [INFO] Analyzing words for concept <what>.\n",
      "2017-02-11 18:12:00,797 [INFO] Analyzing words for concept <white>.\n",
      "2017-02-11 18:12:00,805 [INFO] Analyzing words for concept <who>.\n",
      "2017-02-11 18:12:00,813 [INFO] Analyzing words for concept <wide>.\n",
      "2017-02-11 18:12:00,822 [INFO] Analyzing words for concept <wife>.\n",
      "SEQUENCE CLUSTERING:  95%|█████████▌| 190/200 [00:01<00:00, 124.46it/s]2017-02-11 18:12:00,836 [INFO] Analyzing words for concept <wind>.\n",
      "2017-02-11 18:12:00,845 [INFO] Analyzing words for concept <wing>.\n",
      "2017-02-11 18:12:00,854 [INFO] Analyzing words for concept <wipe>.\n",
      "2017-02-11 18:12:00,863 [INFO] Analyzing words for concept <with>.\n",
      "2017-02-11 18:12:00,870 [INFO] Analyzing words for concept <woman>.\n",
      "2017-02-11 18:12:00,876 [INFO] Analyzing words for concept <woods>.\n",
      "2017-02-11 18:12:00,883 [INFO] Analyzing words for concept <worm>.\n",
      "2017-02-11 18:12:00,890 [INFO] Analyzing words for concept <year>.\n",
      "2017-02-11 18:12:00,895 [INFO] Analyzing words for concept <yellow>.\n",
      "2017-02-11 18:12:00,900 [INFO] Analyzing words for concept <you>.\n",
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCA, threshold=0.6 (precision/recall/f-score):  (0.6856904761904768, 0.9672619047619042, 0.8024941114092562)\n",
      "LexStat, threshold=0.6 (precision/recall/f-score):  (0.9285714285714286, 0.9472619047619046, 0.9378235523440829)\n",
      "EditDist, threshold=0.6 (precision/recall/f-score):  (0.9540476190476191, 0.9114285714285715, 0.9322512535326831)\n",
      "Turchin, threshold=0.6 (precision/recall/f-score):  (0.9310714285714288, 0.9277380952380955, 0.9294017731340009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from lingpy.evaluate.acd import bcubes, diff\n",
    "\n",
    "# do the clustering\n",
    "lex.cluster(method=\"sca\", threshold=0.6, ref=\"cognates_sca\")\n",
    "lex.cluster(method=\"lexstat\", threshold=0.6, ref=\"cognates_lexstat\")\n",
    "lex.cluster(method=\"edit-dist\", threshold=0.6, ref=\"cognates_editdist\")\n",
    "lex.cluster(method=\"turchin\", threshold=0.6, ref=\"cognates_turchin\")\n",
    "\n",
    "# evaluate\n",
    "print(\"SCA, threshold=0.6 (precision/recall/f-score): \", bcubes(lex, \"cogid\", \"cognates_sca\", pprint=False))\n",
    "print(\"LexStat, threshold=0.6 (precision/recall/f-score): \", bcubes(lex, \"cogid\", \"cognates_lexstat\", pprint=False))\n",
    "print(\"EditDist, threshold=0.6 (precision/recall/f-score): \", bcubes(lex, \"cogid\", \"cognates_editdist\", pprint=False))\n",
    "print(\"Turchin, threshold=0.6 (precision/recall/f-score): \", bcubes(lex, \"cogid\", \"cognates_turchin\", pprint=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this particular dataset and with the parameters we used, the get the best f-score is probably the one of the LexStat method (it might differ in your computer, as the wordlist is randomly shuffled). This score is rather high, but we should keep in mind that the dataset is rather small. Just for fun, let's try the same method with higher and lower thresholds, and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SEQUENCE CLUSTERING:   0%|          | 0/200 [00:00<?, ?it/s]2017-02-11 18:12:01,213 [INFO] Analyzing words for concept <I>.\n",
      "2017-02-11 18:12:01,224 [INFO] Analyzing words for concept <all>.\n",
      "2017-02-11 18:12:01,239 [INFO] Analyzing words for concept <and>.\n",
      "2017-02-11 18:12:01,250 [INFO] Analyzing words for concept <animal>.\n",
      "2017-02-11 18:12:01,273 [INFO] Analyzing words for concept <ashes>.\n",
      "2017-02-11 18:12:01,288 [INFO] Analyzing words for concept <at>.\n",
      "2017-02-11 18:12:01,298 [INFO] Analyzing words for concept <back>.\n",
      "2017-02-11 18:12:01,313 [INFO] Analyzing words for concept <bad>.\n",
      "SEQUENCE CLUSTERING:   4%|▍         | 8/200 [00:00<00:02, 68.80it/s]2017-02-11 18:12:01,332 [INFO] Analyzing words for concept <bark>.\n",
      "2017-02-11 18:12:01,352 [INFO] Analyzing words for concept <because>.\n",
      "2017-02-11 18:12:01,368 [INFO] Analyzing words for concept <belly>.\n",
      "2017-02-11 18:12:01,384 [INFO] Analyzing words for concept <big>.\n",
      "2017-02-11 18:12:01,400 [INFO] Analyzing words for concept <bird>.\n",
      "2017-02-11 18:12:01,415 [INFO] Analyzing words for concept <bite>.\n",
      "SEQUENCE CLUSTERING:   7%|▋         | 14/200 [00:00<00:02, 65.43it/s]2017-02-11 18:12:01,435 [INFO] Analyzing words for concept <black>.\n",
      "2017-02-11 18:12:01,452 [INFO] Analyzing words for concept <blood>.\n",
      "2017-02-11 18:12:01,467 [INFO] Analyzing words for concept <blow>.\n",
      "2017-02-11 18:12:01,481 [INFO] Analyzing words for concept <bone>.\n",
      "2017-02-11 18:12:01,496 [INFO] Analyzing words for concept <breast>.\n",
      "2017-02-11 18:12:01,510 [INFO] Analyzing words for concept <breathe>.\n",
      "2017-02-11 18:12:01,529 [INFO] Analyzing words for concept <burn>.\n",
      "SEQUENCE CLUSTERING:  10%|█         | 21/200 [00:00<00:02, 64.00it/s]2017-02-11 18:12:01,549 [INFO] Analyzing words for concept <child>.\n",
      "2017-02-11 18:12:01,566 [INFO] Analyzing words for concept <claw>.\n",
      "2017-02-11 18:12:01,584 [INFO] Analyzing words for concept <cloud>.\n",
      "2017-02-11 18:12:01,600 [INFO] Analyzing words for concept <cold>.\n",
      "2017-02-11 18:12:01,620 [INFO] Analyzing words for concept <come>.\n",
      "2017-02-11 18:12:01,636 [INFO] Analyzing words for concept <count>.\n",
      "SEQUENCE CLUSTERING:  14%|█▎        | 27/200 [00:00<00:02, 60.86it/s]2017-02-11 18:12:01,659 [INFO] Analyzing words for concept <cut>.\n",
      "2017-02-11 18:12:01,677 [INFO] Analyzing words for concept <day>.\n",
      "2017-02-11 18:12:01,689 [INFO] Analyzing words for concept <die>.\n",
      "2017-02-11 18:12:01,704 [INFO] Analyzing words for concept <dig>.\n",
      "2017-02-11 18:12:01,722 [INFO] Analyzing words for concept <dirty>.\n",
      "2017-02-11 18:12:01,740 [INFO] Analyzing words for concept <dog>.\n",
      "SEQUENCE CLUSTERING:  16%|█▋        | 33/200 [00:00<00:02, 60.47it/s]2017-02-11 18:12:01,760 [INFO] Analyzing words for concept <drink>.\n",
      "2017-02-11 18:12:01,775 [INFO] Analyzing words for concept <dry>.\n",
      "2017-02-11 18:12:01,793 [INFO] Analyzing words for concept <dull>.\n",
      "2017-02-11 18:12:01,812 [INFO] Analyzing words for concept <dust>.\n",
      "2017-02-11 18:12:01,830 [INFO] Analyzing words for concept <ear>.\n",
      "2017-02-11 18:12:01,844 [INFO] Analyzing words for concept <earth>.\n",
      "SEQUENCE CLUSTERING:  20%|█▉        | 39/200 [00:00<00:02, 59.74it/s]2017-02-11 18:12:01,864 [INFO] Analyzing words for concept <eat>.\n",
      "2017-02-11 18:12:01,877 [INFO] Analyzing words for concept <egg>.\n",
      "2017-02-11 18:12:01,892 [INFO] Analyzing words for concept <eye>.\n",
      "2017-02-11 18:12:01,905 [INFO] Analyzing words for concept <fall>.\n",
      "2017-02-11 18:12:01,921 [INFO] Analyzing words for concept <far>.\n",
      "2017-02-11 18:12:01,939 [INFO] Analyzing words for concept <father>.\n",
      "2017-02-11 18:12:01,958 [INFO] Analyzing words for concept <feather>.\n",
      "SEQUENCE CLUSTERING:  23%|██▎       | 46/200 [00:00<00:02, 60.11it/s]2017-02-11 18:12:01,978 [INFO] Analyzing words for concept <few>.\n",
      "2017-02-11 18:12:01,997 [INFO] Analyzing words for concept <fight>.\n",
      "2017-02-11 18:12:02,016 [INFO] Analyzing words for concept <fire>.\n",
      "2017-02-11 18:12:02,031 [INFO] Analyzing words for concept <fish>.\n",
      "2017-02-11 18:12:02,048 [INFO] Analyzing words for concept <five>.\n",
      "2017-02-11 18:12:02,067 [INFO] Analyzing words for concept <flow>.\n",
      "SEQUENCE CLUSTERING:  26%|██▌       | 52/200 [00:00<00:02, 58.98it/s]2017-02-11 18:12:02,085 [INFO] Analyzing words for concept <flower>.\n",
      "2017-02-11 18:12:02,103 [INFO] Analyzing words for concept <fly>.\n",
      "2017-02-11 18:12:02,121 [INFO] Analyzing words for concept <fog>.\n",
      "2017-02-11 18:12:02,139 [INFO] Analyzing words for concept <foot>.\n",
      "2017-02-11 18:12:02,157 [INFO] Analyzing words for concept <four>.\n",
      "2017-02-11 18:12:02,170 [INFO] Analyzing words for concept <freeze>.\n",
      "SEQUENCE CLUSTERING:  29%|██▉       | 58/200 [00:00<00:02, 57.74it/s]2017-02-11 18:12:02,193 [INFO] Analyzing words for concept <fruit>.\n",
      "2017-02-11 18:12:02,212 [INFO] Analyzing words for concept <full>.\n",
      "2017-02-11 18:12:02,229 [INFO] Analyzing words for concept <give>.\n",
      "2017-02-11 18:12:02,246 [INFO] Analyzing words for concept <go>.\n",
      "2017-02-11 18:12:02,260 [INFO] Analyzing words for concept <good>.\n",
      "2017-02-11 18:12:02,277 [INFO] Analyzing words for concept <grass>.\n",
      "SEQUENCE CLUSTERING:  32%|███▏      | 64/200 [00:01<00:02, 58.16it/s]2017-02-11 18:12:02,300 [INFO] Analyzing words for concept <grease>.\n",
      "2017-02-11 18:12:02,316 [INFO] Analyzing words for concept <green>.\n",
      "2017-02-11 18:12:02,336 [INFO] Analyzing words for concept <guts>.\n",
      "2017-02-11 18:12:02,354 [INFO] Analyzing words for concept <hair>.\n",
      "2017-02-11 18:12:02,370 [INFO] Analyzing words for concept <hand>.\n",
      "2017-02-11 18:12:02,384 [INFO] Analyzing words for concept <he>.\n",
      "SEQUENCE CLUSTERING:  35%|███▌      | 70/200 [00:01<00:02, 58.07it/s]2017-02-11 18:12:02,398 [INFO] Analyzing words for concept <head>.\n",
      "2017-02-11 18:12:02,412 [INFO] Analyzing words for concept <hear>.\n",
      "2017-02-11 18:12:02,430 [INFO] Analyzing words for concept <heart>.\n",
      "2017-02-11 18:12:02,451 [INFO] Analyzing words for concept <heavy>.\n",
      "2017-02-11 18:12:02,470 [INFO] Analyzing words for concept <here>.\n",
      "2017-02-11 18:12:02,484 [INFO] Analyzing words for concept <hit>.\n",
      "SEQUENCE CLUSTERING:  38%|███▊      | 76/200 [00:01<00:02, 57.46it/s]2017-02-11 18:12:02,506 [INFO] Analyzing words for concept <hold>.\n",
      "2017-02-11 18:12:02,523 [INFO] Analyzing words for concept <horn>.\n",
      "2017-02-11 18:12:02,541 [INFO] Analyzing words for concept <hot>.\n",
      "2017-02-11 18:12:02,557 [INFO] Analyzing words for concept <human>.\n",
      "2017-02-11 18:12:02,577 [INFO] Analyzing words for concept <hunt>.\n",
      "2017-02-11 18:12:02,594 [INFO] Analyzing words for concept <husband>.\n",
      "SEQUENCE CLUSTERING:  41%|████      | 82/200 [00:01<00:02, 56.49it/s]2017-02-11 18:12:02,617 [INFO] Analyzing words for concept <ice>.\n",
      "2017-02-11 18:12:02,631 [INFO] Analyzing words for concept <if>.\n",
      "2017-02-11 18:12:02,645 [INFO] Analyzing words for concept <in>.\n",
      "2017-02-11 18:12:02,658 [INFO] Analyzing words for concept <kill>.\n",
      "2017-02-11 18:12:02,676 [INFO] Analyzing words for concept <knee>.\n",
      "2017-02-11 18:12:02,691 [INFO] Analyzing words for concept <knife>.\n",
      "2017-02-11 18:12:02,707 [INFO] Analyzing words for concept <know>.\n",
      "SEQUENCE CLUSTERING:  44%|████▍     | 89/200 [00:01<00:01, 58.64it/s]2017-02-11 18:12:02,724 [INFO] Analyzing words for concept <lake>.\n",
      "2017-02-11 18:12:02,739 [INFO] Analyzing words for concept <laugh>.\n",
      "2017-02-11 18:12:02,754 [INFO] Analyzing words for concept <leaf>.\n",
      "2017-02-11 18:12:02,775 [INFO] Analyzing words for concept <left>.\n",
      "2017-02-11 18:12:02,790 [INFO] Analyzing words for concept <lie>.\n",
      "2017-02-11 18:12:02,806 [INFO] Analyzing words for concept <liver>.\n",
      "SEQUENCE CLUSTERING:  48%|████▊     | 95/200 [00:01<00:01, 58.85it/s]2017-02-11 18:12:02,826 [INFO] Analyzing words for concept <long>.\n",
      "2017-02-11 18:12:02,841 [INFO] Analyzing words for concept <louse>.\n",
      "2017-02-11 18:12:02,855 [INFO] Analyzing words for concept <man>.\n",
      "2017-02-11 18:12:02,870 [INFO] Analyzing words for concept <many>.\n",
      "2017-02-11 18:12:02,884 [INFO] Analyzing words for concept <meat>.\n",
      "2017-02-11 18:12:02,899 [INFO] Analyzing words for concept <moon>.\n",
      "2017-02-11 18:12:02,916 [INFO] Analyzing words for concept <mother>.\n",
      "SEQUENCE CLUSTERING:  51%|█████     | 102/200 [00:01<00:01, 60.02it/s]2017-02-11 18:12:02,937 [INFO] Analyzing words for concept <mountain>.\n",
      "2017-02-11 18:12:02,955 [INFO] Analyzing words for concept <mouth>.\n",
      "2017-02-11 18:12:02,970 [INFO] Analyzing words for concept <name>.\n",
      "2017-02-11 18:12:02,986 [INFO] Analyzing words for concept <narrow>.\n",
      "2017-02-11 18:12:03,004 [INFO] Analyzing words for concept <near>.\n",
      "2017-02-11 18:12:03,020 [INFO] Analyzing words for concept <neck>.\n",
      "SEQUENCE CLUSTERING:  54%|█████▍    | 108/200 [00:01<00:01, 59.95it/s]2017-02-11 18:12:03,037 [INFO] Analyzing words for concept <new>.\n",
      "2017-02-11 18:12:03,050 [INFO] Analyzing words for concept <night>.\n",
      "2017-02-11 18:12:03,064 [INFO] Analyzing words for concept <nose>.\n",
      "2017-02-11 18:12:03,078 [INFO] Analyzing words for concept <not>.\n",
      "2017-02-11 18:12:03,093 [INFO] Analyzing words for concept <now>.\n",
      "2017-02-11 18:12:03,107 [INFO] Analyzing words for concept <old>.\n",
      "2017-02-11 18:12:03,124 [INFO] Analyzing words for concept <one>.\n",
      "SEQUENCE CLUSTERING:  57%|█████▊    | 115/200 [00:01<00:01, 62.17it/s]2017-02-11 18:12:03,140 [INFO] Analyzing words for concept <other>.\n",
      "2017-02-11 18:12:03,155 [INFO] Analyzing words for concept <path>.\n",
      "2017-02-11 18:12:03,170 [INFO] Analyzing words for concept <play>.\n",
      "2017-02-11 18:12:03,186 [INFO] Analyzing words for concept <pull>.\n",
      "2017-02-11 18:12:03,203 [INFO] Analyzing words for concept <push>.\n",
      "2017-02-11 18:12:03,218 [INFO] Analyzing words for concept <rain>.\n",
      "2017-02-11 18:12:03,235 [INFO] Analyzing words for concept <red>.\n",
      "SEQUENCE CLUSTERING:  61%|██████    | 122/200 [00:02<00:01, 61.90it/s]2017-02-11 18:12:03,255 [INFO] Analyzing words for concept <right>.\n",
      "2017-02-11 18:12:03,272 [INFO] Analyzing words for concept <river>.\n",
      "2017-02-11 18:12:03,290 [INFO] Analyzing words for concept <root>.\n",
      "2017-02-11 18:12:03,306 [INFO] Analyzing words for concept <rotten>.\n",
      "2017-02-11 18:12:03,326 [INFO] Analyzing words for concept <round>.\n",
      "2017-02-11 18:12:03,346 [INFO] Analyzing words for concept <rub>.\n",
      "2017-02-11 18:12:03,365 [INFO] Analyzing words for concept <salt>.\n",
      "SEQUENCE CLUSTERING:  64%|██████▍   | 129/200 [00:02<00:01, 59.23it/s]2017-02-11 18:12:03,384 [INFO] Analyzing words for concept <sand>.\n",
      "2017-02-11 18:12:03,399 [INFO] Analyzing words for concept <say>.\n",
      "2017-02-11 18:12:03,412 [INFO] Analyzing words for concept <scratch>.\n",
      "2017-02-11 18:12:03,432 [INFO] Analyzing words for concept <sea>.\n",
      "2017-02-11 18:12:03,447 [INFO] Analyzing words for concept <see>.\n",
      "2017-02-11 18:12:03,460 [INFO] Analyzing words for concept <seed>.\n",
      "2017-02-11 18:12:03,478 [INFO] Analyzing words for concept <sew>.\n",
      "SEQUENCE CLUSTERING:  68%|██████▊   | 136/200 [00:02<00:01, 60.16it/s]2017-02-11 18:12:03,497 [INFO] Analyzing words for concept <sharp>.\n",
      "2017-02-11 18:12:03,517 [INFO] Analyzing words for concept <short>.\n",
      "2017-02-11 18:12:03,536 [INFO] Analyzing words for concept <sing>.\n",
      "2017-02-11 18:12:03,554 [INFO] Analyzing words for concept <sit>.\n",
      "2017-02-11 18:12:03,570 [INFO] Analyzing words for concept <skin>.\n",
      "2017-02-11 18:12:03,587 [INFO] Analyzing words for concept <sky>.\n",
      "2017-02-11 18:12:03,603 [INFO] Analyzing words for concept <sleep>.\n",
      "SEQUENCE CLUSTERING:  72%|███████▏  | 143/200 [00:02<00:00, 58.72it/s]2017-02-11 18:12:03,623 [INFO] Analyzing words for concept <small>.\n",
      "2017-02-11 18:12:03,641 [INFO] Analyzing words for concept <smell>.\n",
      "2017-02-11 18:12:03,658 [INFO] Analyzing words for concept <smoke>.\n",
      "2017-02-11 18:12:03,674 [INFO] Analyzing words for concept <smooth>.\n",
      "2017-02-11 18:12:03,692 [INFO] Analyzing words for concept <snake>.\n",
      "2017-02-11 18:12:03,711 [INFO] Analyzing words for concept <snow>.\n",
      "SEQUENCE CLUSTERING:  74%|███████▍  | 149/200 [00:02<00:00, 58.05it/s]2017-02-11 18:12:03,728 [INFO] Analyzing words for concept <some>.\n",
      "2017-02-11 18:12:03,746 [INFO] Analyzing words for concept <spit>.\n",
      "2017-02-11 18:12:03,765 [INFO] Analyzing words for concept <split>.\n",
      "2017-02-11 18:12:03,783 [INFO] Analyzing words for concept <squeeze>.\n",
      "2017-02-11 18:12:03,802 [INFO] Analyzing words for concept <stab>.\n",
      "2017-02-11 18:12:03,825 [INFO] Analyzing words for concept <stand>.\n",
      "SEQUENCE CLUSTERING:  78%|███████▊  | 155/200 [00:02<00:00, 55.63it/s]2017-02-11 18:12:03,848 [INFO] Analyzing words for concept <star>.\n",
      "2017-02-11 18:12:03,865 [INFO] Analyzing words for concept <stick>.\n",
      "2017-02-11 18:12:03,884 [INFO] Analyzing words for concept <stone>.\n",
      "2017-02-11 18:12:03,898 [INFO] Analyzing words for concept <straight>.\n",
      "2017-02-11 18:12:03,920 [INFO] Analyzing words for concept <suck>.\n",
      "2017-02-11 18:12:03,934 [INFO] Analyzing words for concept <sun>.\n",
      "SEQUENCE CLUSTERING:  80%|████████  | 161/200 [00:02<00:00, 55.56it/s]2017-02-11 18:12:03,954 [INFO] Analyzing words for concept <swell>.\n",
      "2017-02-11 18:12:03,971 [INFO] Analyzing words for concept <swim>.\n",
      "2017-02-11 18:12:03,987 [INFO] Analyzing words for concept <tail>.\n",
      "2017-02-11 18:12:04,003 [INFO] Analyzing words for concept <that>.\n",
      "2017-02-11 18:12:04,017 [INFO] Analyzing words for concept <there>.\n",
      "2017-02-11 18:12:04,031 [INFO] Analyzing words for concept <they>.\n",
      "2017-02-11 18:12:04,045 [INFO] Analyzing words for concept <thick>.\n",
      "SEQUENCE CLUSTERING:  84%|████████▍ | 168/200 [00:02<00:00, 58.00it/s]2017-02-11 18:12:04,063 [INFO] Analyzing words for concept <thin>.\n",
      "2017-02-11 18:12:04,080 [INFO] Analyzing words for concept <think>.\n",
      "2017-02-11 18:12:04,099 [INFO] Analyzing words for concept <this>.\n",
      "2017-02-11 18:12:04,113 [INFO] Analyzing words for concept <thou>.\n",
      "2017-02-11 18:12:04,125 [INFO] Analyzing words for concept <three>.\n",
      "2017-02-11 18:12:04,139 [INFO] Analyzing words for concept <throw>.\n",
      "2017-02-11 18:12:04,155 [INFO] Analyzing words for concept <tie>.\n",
      "SEQUENCE CLUSTERING:  88%|████████▊ | 175/200 [00:02<00:00, 58.95it/s]2017-02-11 18:12:04,179 [INFO] Analyzing words for concept <tongue>.\n",
      "2017-02-11 18:12:04,195 [INFO] Analyzing words for concept <tooth>.\n",
      "2017-02-11 18:12:04,209 [INFO] Analyzing words for concept <tree>.\n",
      "2017-02-11 18:12:04,226 [INFO] Analyzing words for concept <true>.\n",
      "2017-02-11 18:12:04,246 [INFO] Analyzing words for concept <two>.\n",
      "2017-02-11 18:12:04,261 [INFO] Analyzing words for concept <vomit>.\n",
      "SEQUENCE CLUSTERING:  90%|█████████ | 181/200 [00:03<00:00, 58.34it/s]2017-02-11 18:12:04,286 [INFO] Analyzing words for concept <wash>.\n",
      "2017-02-11 18:12:04,301 [INFO] Analyzing words for concept <water>.\n",
      "2017-02-11 18:12:04,315 [INFO] Analyzing words for concept <we>.\n",
      "2017-02-11 18:12:04,328 [INFO] Analyzing words for concept <wet>.\n",
      "2017-02-11 18:12:04,346 [INFO] Analyzing words for concept <what>.\n",
      "2017-02-11 18:12:04,360 [INFO] Analyzing words for concept <white>.\n",
      "2017-02-11 18:12:04,378 [INFO] Analyzing words for concept <who>.\n",
      "SEQUENCE CLUSTERING:  94%|█████████▍| 188/200 [00:03<00:00, 59.69it/s]2017-02-11 18:12:04,395 [INFO] Analyzing words for concept <wide>.\n",
      "2017-02-11 18:12:04,414 [INFO] Analyzing words for concept <wife>.\n",
      "2017-02-11 18:12:04,429 [INFO] Analyzing words for concept <wind>.\n",
      "2017-02-11 18:12:04,447 [INFO] Analyzing words for concept <wing>.\n",
      "2017-02-11 18:12:04,463 [INFO] Analyzing words for concept <wipe>.\n",
      "2017-02-11 18:12:04,481 [INFO] Analyzing words for concept <with>.\n",
      "SEQUENCE CLUSTERING:  97%|█████████▋| 194/200 [00:03<00:00, 59.48it/s]2017-02-11 18:12:04,496 [INFO] Analyzing words for concept <woman>.\n",
      "2017-02-11 18:12:04,515 [INFO] Analyzing words for concept <woods>.\n",
      "2017-02-11 18:12:04,533 [INFO] Analyzing words for concept <worm>.\n",
      "2017-02-11 18:12:04,551 [INFO] Analyzing words for concept <year>.\n",
      "2017-02-11 18:12:04,567 [INFO] Analyzing words for concept <yellow>.\n",
      "2017-02-11 18:12:04,586 [INFO] Analyzing words for concept <you>.\n",
      "SEQUENCE CLUSTERING:   0%|          | 0/200 [00:00<?, ?it/s]          2017-02-11 18:12:04,615 [INFO] Analyzing words for concept <I>.\n",
      "2017-02-11 18:12:04,627 [INFO] Analyzing words for concept <all>.\n",
      "2017-02-11 18:12:04,644 [INFO] Analyzing words for concept <and>.\n",
      "2017-02-11 18:12:04,656 [INFO] Analyzing words for concept <animal>.\n",
      "2017-02-11 18:12:04,680 [INFO] Analyzing words for concept <ashes>.\n",
      "2017-02-11 18:12:04,696 [INFO] Analyzing words for concept <at>.\n",
      "2017-02-11 18:12:04,709 [INFO] Analyzing words for concept <back>.\n",
      "SEQUENCE CLUSTERING:   4%|▎         | 7/200 [00:00<00:03, 64.31it/s]2017-02-11 18:12:04,728 [INFO] Analyzing words for concept <bad>.\n",
      "2017-02-11 18:12:04,746 [INFO] Analyzing words for concept <bark>.\n",
      "2017-02-11 18:12:04,766 [INFO] Analyzing words for concept <because>.\n",
      "2017-02-11 18:12:04,785 [INFO] Analyzing words for concept <belly>.\n",
      "2017-02-11 18:12:04,801 [INFO] Analyzing words for concept <big>.\n",
      "2017-02-11 18:12:04,820 [INFO] Analyzing words for concept <bird>.\n",
      "SEQUENCE CLUSTERING:   6%|▋         | 13/200 [00:00<00:03, 60.91it/s]2017-02-11 18:12:04,838 [INFO] Analyzing words for concept <bite>.\n",
      "2017-02-11 18:12:04,854 [INFO] Analyzing words for concept <black>.\n",
      "2017-02-11 18:12:04,874 [INFO] Analyzing words for concept <blood>.\n",
      "2017-02-11 18:12:04,890 [INFO] Analyzing words for concept <blow>.\n",
      "2017-02-11 18:12:04,906 [INFO] Analyzing words for concept <bone>.\n",
      "2017-02-11 18:12:04,923 [INFO] Analyzing words for concept <breast>.\n",
      "SEQUENCE CLUSTERING:  10%|▉         | 19/200 [00:00<00:03, 59.68it/s]2017-02-11 18:12:04,942 [INFO] Analyzing words for concept <breathe>.\n",
      "2017-02-11 18:12:04,962 [INFO] Analyzing words for concept <burn>.\n",
      "2017-02-11 18:12:04,979 [INFO] Analyzing words for concept <child>.\n",
      "2017-02-11 18:12:04,998 [INFO] Analyzing words for concept <claw>.\n",
      "2017-02-11 18:12:05,017 [INFO] Analyzing words for concept <cloud>.\n",
      "2017-02-11 18:12:05,033 [INFO] Analyzing words for concept <cold>.\n",
      "SEQUENCE CLUSTERING:  12%|█▎        | 25/200 [00:00<00:03, 57.63it/s]2017-02-11 18:12:05,055 [INFO] Analyzing words for concept <come>.\n",
      "2017-02-11 18:12:05,072 [INFO] Analyzing words for concept <count>.\n",
      "2017-02-11 18:12:05,090 [INFO] Analyzing words for concept <cut>.\n",
      "2017-02-11 18:12:05,110 [INFO] Analyzing words for concept <day>.\n",
      "2017-02-11 18:12:05,121 [INFO] Analyzing words for concept <die>.\n",
      "2017-02-11 18:12:05,138 [INFO] Analyzing words for concept <dig>.\n",
      "SEQUENCE CLUSTERING:  16%|█▌        | 31/200 [00:00<00:02, 57.34it/s]2017-02-11 18:12:05,161 [INFO] Analyzing words for concept <dirty>.\n",
      "2017-02-11 18:12:05,179 [INFO] Analyzing words for concept <dog>.\n",
      "2017-02-11 18:12:05,199 [INFO] Analyzing words for concept <drink>.\n",
      "2017-02-11 18:12:05,213 [INFO] Analyzing words for concept <dry>.\n",
      "2017-02-11 18:12:05,232 [INFO] Analyzing words for concept <dull>.\n",
      "2017-02-11 18:12:05,251 [INFO] Analyzing words for concept <dust>.\n",
      "SEQUENCE CLUSTERING:  18%|█▊        | 37/200 [00:00<00:02, 56.38it/s]2017-02-11 18:12:05,273 [INFO] Analyzing words for concept <ear>.\n",
      "2017-02-11 18:12:05,289 [INFO] Analyzing words for concept <earth>.\n",
      "2017-02-11 18:12:05,306 [INFO] Analyzing words for concept <eat>.\n",
      "2017-02-11 18:12:05,319 [INFO] Analyzing words for concept <egg>.\n",
      "2017-02-11 18:12:05,334 [INFO] Analyzing words for concept <eye>.\n",
      "2017-02-11 18:12:05,353 [INFO] Analyzing words for concept <fall>.\n",
      "2017-02-11 18:12:05,368 [INFO] Analyzing words for concept <far>.\n",
      "SEQUENCE CLUSTERING:  22%|██▏       | 44/200 [00:00<00:02, 57.67it/s]2017-02-11 18:12:05,387 [INFO] Analyzing words for concept <father>.\n",
      "2017-02-11 18:12:05,407 [INFO] Analyzing words for concept <feather>.\n",
      "2017-02-11 18:12:05,424 [INFO] Analyzing words for concept <few>.\n",
      "2017-02-11 18:12:05,443 [INFO] Analyzing words for concept <fight>.\n",
      "2017-02-11 18:12:05,462 [INFO] Analyzing words for concept <fire>.\n",
      "2017-02-11 18:12:05,480 [INFO] Analyzing words for concept <fish>.\n",
      "SEQUENCE CLUSTERING:  25%|██▌       | 50/200 [00:00<00:02, 56.21it/s]2017-02-11 18:12:05,501 [INFO] Analyzing words for concept <five>.\n",
      "2017-02-11 18:12:05,518 [INFO] Analyzing words for concept <flow>.\n",
      "2017-02-11 18:12:05,535 [INFO] Analyzing words for concept <flower>.\n",
      "2017-02-11 18:12:05,553 [INFO] Analyzing words for concept <fly>.\n",
      "2017-02-11 18:12:05,571 [INFO] Analyzing words for concept <fog>.\n",
      "2017-02-11 18:12:05,589 [INFO] Analyzing words for concept <foot>.\n",
      "SEQUENCE CLUSTERING:  28%|██▊       | 56/200 [00:00<00:02, 55.91it/s]2017-02-11 18:12:05,608 [INFO] Analyzing words for concept <four>.\n",
      "2017-02-11 18:12:05,624 [INFO] Analyzing words for concept <freeze>.\n",
      "2017-02-11 18:12:05,643 [INFO] Analyzing words for concept <fruit>.\n",
      "2017-02-11 18:12:05,664 [INFO] Analyzing words for concept <full>.\n",
      "2017-02-11 18:12:05,682 [INFO] Analyzing words for concept <give>.\n",
      "2017-02-11 18:12:05,700 [INFO] Analyzing words for concept <go>.\n",
      "SEQUENCE CLUSTERING:  31%|███       | 62/200 [00:01<00:02, 55.96it/s]2017-02-11 18:12:05,717 [INFO] Analyzing words for concept <good>.\n",
      "2017-02-11 18:12:05,734 [INFO] Analyzing words for concept <grass>.\n",
      "2017-02-11 18:12:05,750 [INFO] Analyzing words for concept <grease>.\n",
      "2017-02-11 18:12:05,766 [INFO] Analyzing words for concept <green>.\n",
      "2017-02-11 18:12:05,787 [INFO] Analyzing words for concept <guts>.\n",
      "2017-02-11 18:12:05,805 [INFO] Analyzing words for concept <hair>.\n",
      "SEQUENCE CLUSTERING:  34%|███▍      | 68/200 [00:01<00:02, 55.74it/s]2017-02-11 18:12:05,824 [INFO] Analyzing words for concept <hand>.\n",
      "2017-02-11 18:12:05,837 [INFO] Analyzing words for concept <he>.\n",
      "2017-02-11 18:12:05,848 [INFO] Analyzing words for concept <head>.\n",
      "2017-02-11 18:12:05,862 [INFO] Analyzing words for concept <hear>.\n",
      "2017-02-11 18:12:05,878 [INFO] Analyzing words for concept <heart>.\n",
      "2017-02-11 18:12:05,899 [INFO] Analyzing words for concept <heavy>.\n",
      "2017-02-11 18:12:05,919 [INFO] Analyzing words for concept <here>.\n",
      "SEQUENCE CLUSTERING:  38%|███▊      | 75/200 [00:01<00:02, 57.31it/s]2017-02-11 18:12:05,955 [INFO] Analyzing words for concept <hit>.\n",
      "2017-02-11 18:12:05,979 [INFO] Analyzing words for concept <hold>.\n",
      "2017-02-11 18:12:05,999 [INFO] Analyzing words for concept <horn>.\n",
      "2017-02-11 18:12:06,020 [INFO] Analyzing words for concept <hot>.\n",
      "2017-02-11 18:12:06,038 [INFO] Analyzing words for concept <human>.\n",
      "2017-02-11 18:12:06,058 [INFO] Analyzing words for concept <hunt>.\n",
      "SEQUENCE CLUSTERING:  40%|████      | 81/200 [00:01<00:02, 51.95it/s]2017-02-11 18:12:06,078 [INFO] Analyzing words for concept <husband>.\n",
      "2017-02-11 18:12:06,097 [INFO] Analyzing words for concept <ice>.\n",
      "2017-02-11 18:12:06,112 [INFO] Analyzing words for concept <if>.\n",
      "2017-02-11 18:12:06,126 [INFO] Analyzing words for concept <in>.\n",
      "2017-02-11 18:12:06,140 [INFO] Analyzing words for concept <kill>.\n",
      "2017-02-11 18:12:06,160 [INFO] Analyzing words for concept <knee>.\n",
      "2017-02-11 18:12:06,176 [INFO] Analyzing words for concept <knife>.\n",
      "SEQUENCE CLUSTERING:  44%|████▍     | 88/200 [00:01<00:02, 54.16it/s]2017-02-11 18:12:06,196 [INFO] Analyzing words for concept <know>.\n",
      "2017-02-11 18:12:06,214 [INFO] Analyzing words for concept <lake>.\n",
      "2017-02-11 18:12:06,232 [INFO] Analyzing words for concept <laugh>.\n",
      "2017-02-11 18:12:06,246 [INFO] Analyzing words for concept <leaf>.\n",
      "2017-02-11 18:12:06,262 [INFO] Analyzing words for concept <left>.\n",
      "2017-02-11 18:12:06,279 [INFO] Analyzing words for concept <lie>.\n",
      "SEQUENCE CLUSTERING:  47%|████▋     | 94/200 [00:01<00:01, 55.38it/s]2017-02-11 18:12:06,298 [INFO] Analyzing words for concept <liver>.\n",
      "2017-02-11 18:12:06,316 [INFO] Analyzing words for concept <long>.\n",
      "2017-02-11 18:12:06,332 [INFO] Analyzing words for concept <louse>.\n",
      "2017-02-11 18:12:06,346 [INFO] Analyzing words for concept <man>.\n",
      "2017-02-11 18:12:06,363 [INFO] Analyzing words for concept <many>.\n",
      "2017-02-11 18:12:06,379 [INFO] Analyzing words for concept <meat>.\n",
      "2017-02-11 18:12:06,394 [INFO] Analyzing words for concept <moon>.\n",
      "SEQUENCE CLUSTERING:  50%|█████     | 101/200 [00:01<00:01, 56.81it/s]2017-02-11 18:12:06,414 [INFO] Analyzing words for concept <mother>.\n",
      "2017-02-11 18:12:06,432 [INFO] Analyzing words for concept <mountain>.\n",
      "2017-02-11 18:12:06,450 [INFO] Analyzing words for concept <mouth>.\n",
      "2017-02-11 18:12:06,466 [INFO] Analyzing words for concept <name>.\n",
      "2017-02-11 18:12:06,482 [INFO] Analyzing words for concept <narrow>.\n",
      "2017-02-11 18:12:06,502 [INFO] Analyzing words for concept <near>.\n",
      "SEQUENCE CLUSTERING:  54%|█████▎    | 107/200 [00:01<00:01, 56.43it/s]2017-02-11 18:12:06,522 [INFO] Analyzing words for concept <neck>.\n",
      "2017-02-11 18:12:06,537 [INFO] Analyzing words for concept <new>.\n",
      "2017-02-11 18:12:06,550 [INFO] Analyzing words for concept <night>.\n",
      "2017-02-11 18:12:06,565 [INFO] Analyzing words for concept <nose>.\n",
      "2017-02-11 18:12:06,580 [INFO] Analyzing words for concept <not>.\n",
      "2017-02-11 18:12:06,598 [INFO] Analyzing words for concept <now>.\n",
      "2017-02-11 18:12:06,617 [INFO] Analyzing words for concept <old>.\n",
      "SEQUENCE CLUSTERING:  57%|█████▋    | 114/200 [00:02<00:01, 57.77it/s]2017-02-11 18:12:06,637 [INFO] Analyzing words for concept <one>.\n",
      "2017-02-11 18:12:06,651 [INFO] Analyzing words for concept <other>.\n",
      "2017-02-11 18:12:06,669 [INFO] Analyzing words for concept <path>.\n",
      "2017-02-11 18:12:06,685 [INFO] Analyzing words for concept <play>.\n",
      "2017-02-11 18:12:06,702 [INFO] Analyzing words for concept <pull>.\n",
      "2017-02-11 18:12:06,719 [INFO] Analyzing words for concept <push>.\n",
      "SEQUENCE CLUSTERING:  60%|██████    | 120/200 [00:02<00:01, 57.39it/s]2017-02-11 18:12:06,742 [INFO] Analyzing words for concept <rain>.\n",
      "2017-02-11 18:12:06,759 [INFO] Analyzing words for concept <red>.\n",
      "2017-02-11 18:12:06,774 [INFO] Analyzing words for concept <right>.\n",
      "2017-02-11 18:12:06,791 [INFO] Analyzing words for concept <river>.\n",
      "2017-02-11 18:12:06,810 [INFO] Analyzing words for concept <root>.\n",
      "2017-02-11 18:12:06,828 [INFO] Analyzing words for concept <rotten>.\n",
      "SEQUENCE CLUSTERING:  63%|██████▎   | 126/200 [00:02<00:01, 56.91it/s]2017-02-11 18:12:06,850 [INFO] Analyzing words for concept <round>.\n",
      "2017-02-11 18:12:06,872 [INFO] Analyzing words for concept <rub>.\n",
      "2017-02-11 18:12:06,892 [INFO] Analyzing words for concept <salt>.\n",
      "2017-02-11 18:12:06,910 [INFO] Analyzing words for concept <sand>.\n",
      "2017-02-11 18:12:06,926 [INFO] Analyzing words for concept <say>.\n",
      "2017-02-11 18:12:06,939 [INFO] Analyzing words for concept <scratch>.\n",
      "SEQUENCE CLUSTERING:  66%|██████▌   | 132/200 [00:02<00:01, 55.70it/s]2017-02-11 18:12:06,964 [INFO] Analyzing words for concept <sea>.\n",
      "2017-02-11 18:12:06,980 [INFO] Analyzing words for concept <see>.\n",
      "2017-02-11 18:12:06,995 [INFO] Analyzing words for concept <seed>.\n",
      "2017-02-11 18:12:07,016 [INFO] Analyzing words for concept <sew>.\n",
      "2017-02-11 18:12:07,032 [INFO] Analyzing words for concept <sharp>.\n",
      "2017-02-11 18:12:07,052 [INFO] Analyzing words for concept <short>.\n",
      "SEQUENCE CLUSTERING:  69%|██████▉   | 138/200 [00:02<00:01, 55.16it/s]2017-02-11 18:12:07,075 [INFO] Analyzing words for concept <sing>.\n",
      "2017-02-11 18:12:07,094 [INFO] Analyzing words for concept <sit>.\n",
      "2017-02-11 18:12:07,111 [INFO] Analyzing words for concept <skin>.\n",
      "2017-02-11 18:12:07,126 [INFO] Analyzing words for concept <sky>.\n",
      "2017-02-11 18:12:07,142 [INFO] Analyzing words for concept <sleep>.\n",
      "2017-02-11 18:12:07,159 [INFO] Analyzing words for concept <small>.\n",
      "SEQUENCE CLUSTERING:  72%|███████▏  | 144/200 [00:02<00:01, 55.70it/s]2017-02-11 18:12:07,181 [INFO] Analyzing words for concept <smell>.\n",
      "2017-02-11 18:12:07,201 [INFO] Analyzing words for concept <smoke>.\n",
      "2017-02-11 18:12:07,216 [INFO] Analyzing words for concept <smooth>.\n",
      "2017-02-11 18:12:07,234 [INFO] Analyzing words for concept <snake>.\n",
      "2017-02-11 18:12:07,254 [INFO] Analyzing words for concept <snow>.\n",
      "2017-02-11 18:12:07,268 [INFO] Analyzing words for concept <some>.\n",
      "SEQUENCE CLUSTERING:  75%|███████▌  | 150/200 [00:02<00:00, 55.60it/s]2017-02-11 18:12:07,289 [INFO] Analyzing words for concept <spit>.\n",
      "2017-02-11 18:12:07,308 [INFO] Analyzing words for concept <split>.\n",
      "2017-02-11 18:12:07,328 [INFO] Analyzing words for concept <squeeze>.\n",
      "2017-02-11 18:12:07,347 [INFO] Analyzing words for concept <stab>.\n",
      "2017-02-11 18:12:07,370 [INFO] Analyzing words for concept <stand>.\n",
      "2017-02-11 18:12:07,387 [INFO] Analyzing words for concept <star>.\n",
      "SEQUENCE CLUSTERING:  78%|███████▊  | 156/200 [00:02<00:00, 53.71it/s]2017-02-11 18:12:07,408 [INFO] Analyzing words for concept <stick>.\n",
      "2017-02-11 18:12:07,425 [INFO] Analyzing words for concept <stone>.\n",
      "2017-02-11 18:12:07,442 [INFO] Analyzing words for concept <straight>.\n",
      "2017-02-11 18:12:07,466 [INFO] Analyzing words for concept <suck>.\n",
      "2017-02-11 18:12:07,482 [INFO] Analyzing words for concept <sun>.\n",
      "2017-02-11 18:12:07,497 [INFO] Analyzing words for concept <swell>.\n",
      "SEQUENCE CLUSTERING:  81%|████████  | 162/200 [00:02<00:00, 54.15it/s]2017-02-11 18:12:07,518 [INFO] Analyzing words for concept <swim>.\n",
      "2017-02-11 18:12:07,534 [INFO] Analyzing words for concept <tail>.\n",
      "2017-02-11 18:12:07,551 [INFO] Analyzing words for concept <that>.\n",
      "2017-02-11 18:12:07,566 [INFO] Analyzing words for concept <there>.\n",
      "2017-02-11 18:12:07,580 [INFO] Analyzing words for concept <they>.\n",
      "2017-02-11 18:12:07,593 [INFO] Analyzing words for concept <thick>.\n",
      "2017-02-11 18:12:07,611 [INFO] Analyzing words for concept <thin>.\n",
      "SEQUENCE CLUSTERING:  84%|████████▍ | 169/200 [00:03<00:00, 56.23it/s]2017-02-11 18:12:07,631 [INFO] Analyzing words for concept <think>.\n",
      "2017-02-11 18:12:07,651 [INFO] Analyzing words for concept <this>.\n",
      "2017-02-11 18:12:07,667 [INFO] Analyzing words for concept <thou>.\n",
      "2017-02-11 18:12:07,680 [INFO] Analyzing words for concept <three>.\n",
      "2017-02-11 18:12:07,693 [INFO] Analyzing words for concept <throw>.\n",
      "2017-02-11 18:12:07,709 [INFO] Analyzing words for concept <tie>.\n",
      "SEQUENCE CLUSTERING:  88%|████████▊ | 175/200 [00:03<00:00, 57.26it/s]2017-02-11 18:12:07,730 [INFO] Analyzing words for concept <tongue>.\n",
      "2017-02-11 18:12:07,746 [INFO] Analyzing words for concept <tooth>.\n",
      "2017-02-11 18:12:07,761 [INFO] Analyzing words for concept <tree>.\n",
      "2017-02-11 18:12:07,778 [INFO] Analyzing words for concept <true>.\n",
      "2017-02-11 18:12:07,805 [INFO] Analyzing words for concept <two>.\n",
      "2017-02-11 18:12:07,820 [INFO] Analyzing words for concept <vomit>.\n",
      "SEQUENCE CLUSTERING:  90%|█████████ | 181/200 [00:03<00:00, 56.46it/s]2017-02-11 18:12:07,841 [INFO] Analyzing words for concept <wash>.\n",
      "2017-02-11 18:12:07,858 [INFO] Analyzing words for concept <water>.\n",
      "2017-02-11 18:12:07,874 [INFO] Analyzing words for concept <we>.\n",
      "2017-02-11 18:12:07,887 [INFO] Analyzing words for concept <wet>.\n",
      "2017-02-11 18:12:07,905 [INFO] Analyzing words for concept <what>.\n",
      "2017-02-11 18:12:07,920 [INFO] Analyzing words for concept <white>.\n",
      "SEQUENCE CLUSTERING:  94%|█████████▎| 187/200 [00:03<00:00, 57.48it/s]2017-02-11 18:12:07,940 [INFO] Analyzing words for concept <who>.\n",
      "2017-02-11 18:12:07,953 [INFO] Analyzing words for concept <wide>.\n",
      "2017-02-11 18:12:07,971 [INFO] Analyzing words for concept <wife>.\n",
      "2017-02-11 18:12:07,988 [INFO] Analyzing words for concept <wind>.\n",
      "2017-02-11 18:12:08,006 [INFO] Analyzing words for concept <wing>.\n",
      "2017-02-11 18:12:08,023 [INFO] Analyzing words for concept <wipe>.\n",
      "SEQUENCE CLUSTERING:  96%|█████████▋| 193/200 [00:03<00:00, 57.46it/s]2017-02-11 18:12:08,044 [INFO] Analyzing words for concept <with>.\n",
      "2017-02-11 18:12:08,058 [INFO] Analyzing words for concept <woman>.\n",
      "2017-02-11 18:12:08,079 [INFO] Analyzing words for concept <woods>.\n",
      "2017-02-11 18:12:08,097 [INFO] Analyzing words for concept <worm>.\n",
      "2017-02-11 18:12:08,116 [INFO] Analyzing words for concept <year>.\n",
      "2017-02-11 18:12:08,132 [INFO] Analyzing words for concept <yellow>.\n",
      "SEQUENCE CLUSTERING: 100%|█████████▉| 199/200 [00:03<00:00, 56.70it/s]2017-02-11 18:12:08,155 [INFO] Analyzing words for concept <you>.\n",
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexStat, threshold=0.3 (precision/recall/f-score):  (0.9957142857142857, 0.8897619047619045, 0.9397611531037467)\n",
      "LexStat, threshold=0.9 (precision/recall/f-score):  (0.5130170068027222, 0.9804761904761902, 0.6735898916659002)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lex.cluster(method=\"lexstat\", threshold=0.3, ref=\"cognates_lexstat03\")\n",
    "lex.cluster(method=\"lexstat\", threshold=0.9, ref=\"cognates_lexstat09\")\n",
    "\n",
    "print(\"LexStat, threshold=0.3 (precision/recall/f-score): \", bcubes(lex, \"cogid\", \"cognates_lexstat03\", pprint=False))\n",
    "print(\"LexStat, threshold=0.9 (precision/recall/f-score): \", bcubes(lex, \"cogid\", \"cognates_lexstat09\", pprint=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With this particular dataset, you probably got a higher score with a lower threshold. There is no golden rule here, as each method will require different parameters and a higher score may not necessarily be what you are looking for (for example, if the golden standard is biased and you are using lingpy precisely to understand how). Once more, lingpy is a tool for computer *assisted* historical linguistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alignments and Philogeny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we know that our data has been properly analyzed with good cognate scores, lets align it, using the Alignments class. We can directly initialize it from the LexStat object, but we need to pass the “cognates” column as “ref” (this tells LingPy, where to search for cognate sets which are then multiply aligned), and then, we call the function align, using the defaults for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alm = lingpy.Alignments(lex, ref='cognates_lexstat')\n",
    "alm.align()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you want to see the results of this analysis, you need to write them to file. But there, it is also difficult to see the alignments, since they are in a TSV-file in just another column, called “alignment” as a default. Another possibility is to write data to HTML format instead. This means you can’t re-import the data into LingPy, but you can inspect the results. This will create the file KSL.html which you can inspect by loading it in your webbrowser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-11 18:12:09,272 [INFO] Data has been written to file </tmp/tmp7lg6no9l.alm>.\n",
      "2017-02-11 18:12:09,405 [INFO] Data has been written to file <align_KSL.html>.\n"
     ]
    }
   ],
   "source": [
    "alm.output('html', filename=\"align_KSL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, let’s make a tree of the data. This is very straightforward by passing the “cognates” column as a reference to the calculate function. Printing of the resulting “tree” which is created as an attribute of the LexStat object, is possible with help of LingPy’s Tree class: Well, given that there are unrelated languages in our sample, we should be careful with any interpretations here, but let’s at least be glad that the algorithm did cluster the Indo-European languages all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-11 18:12:09,565 [INFO] Successfully calculated tree.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          /-Hawaiian\n",
      "-root----|\n",
      "         |          /-Navajo\n",
      "          \\edge.4--|\n",
      "                   |          /-Turkish\n",
      "                    \\edge.3--|\n",
      "                             |          /-Albanian\n",
      "                              \\edge.2--|\n",
      "                                       |          /-French\n",
      "                                        \\edge.1--|\n",
      "                                                 |          /-English\n",
      "                                                  \\edge.0--|\n",
      "                                                            \\-German\n"
     ]
    }
   ],
   "source": [
    "lex.calculate('tree', ref='cognates_lexstat')\n",
    "print(lex.tree.asciiArt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### calculate lexicostatistic distances between languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### reconstruct language phylogenies using basic cluster algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((Norwegian:0.18,(Swedish:0.12,Icelandic:0.28):0.20):0.17,English:-0.01):0.16,Dutch:0.16);\n"
     ]
    }
   ],
   "source": [
    "from lingpy.algorithm import squareform\n",
    "languages = ['Norwegian','Swedish','Icelandic','Dutch','English']\n",
    "distances = squareform([0.5,0.67,0.8,0.2,0.4,0.7,0.6,0.8,0.8,0.3])\n",
    "tree = lingpy.neighbor(distances,languages)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Print the tree in ASCII-Art to the screen (using PyCogent’s tree class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              /-Norwegian\n",
      "                    /edge.1--|\n",
      "                   |         |          /-Swedish\n",
      "          /edge.2--|          \\edge.0--|\n",
      "         |         |                    \\-Icelandic\n",
      "-root----|         |\n",
      "         |          \\-English\n",
      "         |\n",
      "          \\-Dutch\n"
     ]
    }
   ],
   "source": [
    "n_tree = lingpy.Tree(tree)\n",
    "print(n_tree.asciiArt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# What else?\n",
    "\n",
    "- generate random sequences\n",
    "- concept comparison\n",
    "- Compute the Infomap clustering analysis of the data (requires...)\n",
    "- affinity propagaion (requires sklearn to run the analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For advanced users, you can manually inspect all the builtin parameters with the code below. Please note that this is only intended for debugging and studying, as the standard way to communicate with `rcParams` (which is just a Python dictionary) is by using the `lingpy.rc()` function as we did above.\n",
    "\n",
    "For multiple alignment, we are here using progressive alignment analysis (method `prog_align()`) in contrast to library-based progressive alignment analysis (method `lib_align()`); the results shouldn't differ for most test cases and you can study their difference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lingpy.settings\n",
    "lingpy.settings.rcParams\n",
    "\n",
    "ipa = lingpy.sampa2uni('tsOyg@') # /ʦɔyɡə/ in X-SAMPA\n",
    "tokens = lingpy.ipa2tokens(ipa) # Tokenize IPA-encoded strings.\n",
    "classes = lingpy.tokens2class(tokens, 'sca') # Convert tokenized IPA strings into their respective class strings.\n",
    "prosody = lingpy.prosodic_string(tokens)\n",
    "prosodic_weights = lingpy.prosodic_weights(prosody)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
